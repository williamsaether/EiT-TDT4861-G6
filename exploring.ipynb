{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "cb6f68fa",
      "metadata": {},
      "source": [
        "# RSCD Road Surface Classification\n",
        "\n",
        "This notebook trains an image classifier to predict road-surface classes using the RSCD dataset.\n",
        "It uses the train split (folders per class) and parses labels from filenames for validation/test.\n",
        "\n",
        "Notes:\n",
        "- The full dataset is large; use the `MAX_*_SAMPLES` knobs for quick experiments.\n",
        "- Pretrained weights require an existing local cache or internet access; set `USE_PRETRAINED` accordingly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "3eafb6e8",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2026-02-03T18:45:18.076696Z",
          "start_time": "2026-02-03T18:45:18.066103Z"
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader, Subset, WeightedRandomSampler\n",
        "from torchvision import datasets, transforms, models\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "DATA_ROOT = Path(\"data\") / \"RSCD dataset-1million\"\n",
        "TRAIN_DIR = DATA_ROOT / \"train\"\n",
        "VAL_DIR = DATA_ROOT / \"vali_20k\"\n",
        "TEST_DIR = DATA_ROOT / \"test_50k\"\n",
        "\n",
        "SEED = 42\n",
        "BATCH_SIZE = 64\n",
        "IMAGE_SIZE = 224\n",
        "NUM_WORKERS = 0\n",
        "EPOCHS = 3\n",
        "LR = 3e-4\n",
        "WEIGHT_DECAY = 1e-4\n",
        "USE_PRETRAINED = False\n",
        "\n",
        "MAX_TRAIN_SAMPLES = None\n",
        "MAX_VAL_SAMPLES = None\n",
        "MAX_TEST_SAMPLES = None\n",
        "\n",
        "CLASS_NAMES = sorted([p.name for p in TRAIN_DIR.iterdir() if p.is_dir()]) if TRAIN_DIR.exists() else []\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(SEED)\n",
        "device = torch.device(\"mps\") if torch.backends.mps.is_available() else (\n",
        "    torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        ")\n",
        "if torch.cuda.is_available():\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "device\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "b0957487",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2026-02-03T18:45:18.952161Z",
          "start_time": "2026-02-03T18:45:18.946144Z"
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(27,\n",
              " 12,\n",
              " ['dry_asphalt',\n",
              "  'dry_gravel',\n",
              "  'dry_mud',\n",
              "  'fresh_snow',\n",
              "  'ice',\n",
              "  'melted_snow',\n",
              "  'water_asphalt',\n",
              "  'water_gravel',\n",
              "  'water_mud',\n",
              "  'wet_asphalt',\n",
              "  'wet_gravel',\n",
              "  'wet_mud'])"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "assert TRAIN_DIR.exists(), f\"Missing train directory: {TRAIN_DIR}\"\n",
        "assert VAL_DIR.exists(), f\"Missing val directory: {VAL_DIR}\"\n",
        "assert TEST_DIR.exists(), f\"Missing test directory: {TEST_DIR}\"\n",
        "\n",
        "def normalize_label(label: str):\n",
        "    if \"concrete\" in label:\n",
        "        return None\n",
        "    return label.replace(\"_severe\", \"\").replace(\"_slight\", \"\").replace(\"_smooth\", \"\")\n",
        "\n",
        "class_names = []\n",
        "for name in CLASS_NAMES:\n",
        "    merged = normalize_label(name)\n",
        "    if merged is not None and merged not in class_names:\n",
        "        class_names.append(merged)\n",
        "\n",
        "num_classes = len(class_names)\n",
        "(len(CLASS_NAMES), num_classes, class_names)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "e325e1b2",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2026-02-03T18:45:29.025911Z",
          "start_time": "2026-02-03T18:45:19.901035Z"
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(703279, 13620, 34550, 12)"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mean = [0.485, 0.456, 0.406]\n",
        "std = [0.229, 0.224, 0.225]\n",
        "\n",
        "train_tfms = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(IMAGE_SIZE, scale=(0.8, 1.0)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.05),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std),\n",
        "])\n",
        "\n",
        "eval_tfms = transforms.Compose([\n",
        "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std),\n",
        "])\n",
        "\n",
        "base_train_ds = datasets.ImageFolder(TRAIN_DIR, transform=train_tfms)\n",
        "orig_idx_to_class = {v: k for k, v in base_train_ds.class_to_idx.items()}\n",
        "class_to_idx = {name: i for i, name in enumerate(class_names)}\n",
        "idx_to_class = {v: k for k, v in class_to_idx.items()}\n",
        "\n",
        "class MergedImageFolder(Dataset):\n",
        "    def __init__(self, base_ds, idx_to_orig_class, class_to_idx):\n",
        "        self.base_ds = base_ds\n",
        "        self.indices = []\n",
        "        self.targets = []\n",
        "        for sample_idx, (_, orig_target) in enumerate(base_ds.samples):\n",
        "            orig_label = idx_to_orig_class[orig_target]\n",
        "            merged_label = normalize_label(orig_label)\n",
        "            if merged_label is None:\n",
        "                continue\n",
        "            self.indices.append(sample_idx)\n",
        "            self.targets.append(class_to_idx[merged_label])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.indices)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample_idx = self.indices[idx]\n",
        "        img, _ = self.base_ds[sample_idx]\n",
        "        return img, self.targets[idx]\n",
        "\n",
        "train_ds = MergedImageFolder(base_train_ds, orig_idx_to_class, class_to_idx)\n",
        "\n",
        "def parse_label_from_filename(path: Path) -> str:\n",
        "    stem = path.stem\n",
        "    if \"-\" not in stem:\n",
        "        return stem\n",
        "    _, label = stem.split(\"-\", 1)\n",
        "    return label.replace(\"-\", \"_\")\n",
        "\n",
        "class RSCDFlatDataset(Dataset):\n",
        "    def __init__(self, root_dir: Path, transform=None, class_to_idx=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.class_to_idx = class_to_idx or {}\n",
        "        all_samples = sorted([p for p in root_dir.iterdir() if p.is_file()])\n",
        "        self.samples = [\n",
        "            p for p in all_samples\n",
        "            if normalize_label(parse_label_from_filename(p)) in self.class_to_idx\n",
        "        ]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path = self.samples[idx]\n",
        "        label_name = normalize_label(parse_label_from_filename(path))\n",
        "        label = self.class_to_idx[label_name]\n",
        "        img = datasets.folder.default_loader(path)\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img, label\n",
        "\n",
        "val_ds = RSCDFlatDataset(VAL_DIR, transform=eval_tfms, class_to_idx=class_to_idx)\n",
        "test_ds = RSCDFlatDataset(TEST_DIR, transform=eval_tfms, class_to_idx=class_to_idx)\n",
        "\n",
        "len(train_ds), len(val_ds), len(test_ds), num_classes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "5c6b5ec9",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2026-02-03T18:45:37.626851Z",
          "start_time": "2026-02-03T18:45:37.206586Z"
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([64, 3, 224, 224])"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def maybe_subset(ds, max_samples, seed=42):\n",
        "    if max_samples is None or max_samples >= len(ds):\n",
        "        return ds\n",
        "    rng = np.random.default_rng(seed)\n",
        "    indices = rng.choice(len(ds), size=max_samples, replace=False)\n",
        "    return Subset(ds, indices)\n",
        "\n",
        "def get_targets(dataset):\n",
        "    if isinstance(dataset, Subset):\n",
        "        return np.array(dataset.dataset.targets)[dataset.indices]\n",
        "    return np.array(dataset.targets)\n",
        "\n",
        "train_ds_sub = maybe_subset(train_ds, MAX_TRAIN_SAMPLES, SEED)\n",
        "val_ds_sub = maybe_subset(val_ds, MAX_VAL_SAMPLES, SEED)\n",
        "test_ds_sub = maybe_subset(test_ds, MAX_TEST_SAMPLES, SEED)\n",
        "\n",
        "train_targets = get_targets(train_ds_sub)\n",
        "class_counts = np.bincount(train_targets, minlength=num_classes)\n",
        "class_counts = np.maximum(class_counts, 1)\n",
        "sample_weights = (1.0 / class_counts)[train_targets]\n",
        "train_sampler = WeightedRandomSampler(\n",
        "    weights=torch.tensor(sample_weights, dtype=torch.double),\n",
        "    num_samples=len(sample_weights),\n",
        "    replacement=True,\n",
        ")\n",
        "\n",
        "loader_kwargs = dict(batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, pin_memory=True)\n",
        "train_loader = DataLoader(train_ds_sub, sampler=train_sampler, shuffle=False, **loader_kwargs)\n",
        "val_loader = DataLoader(val_ds_sub, shuffle=False, **loader_kwargs)\n",
        "test_loader = DataLoader(test_ds_sub, shuffle=False, **loader_kwargs)\n",
        "\n",
        "next(iter(train_loader))[0].shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "e4f5606c",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2026-02-03T18:45:40.394781Z",
          "start_time": "2026-02-03T18:45:38.714743Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Avg train batch load time: 0.335s\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "def time_dataloader(loader, max_batches=5):\n",
        "    start = time.time()\n",
        "    for i, _ in enumerate(loader):\n",
        "        if i + 1 >= max_batches:\n",
        "            break\n",
        "    elapsed = time.time() - start\n",
        "    return elapsed / max_batches\n",
        "\n",
        "avg_batch_time = time_dataloader(train_loader, max_batches=5)\n",
        "print(f'Avg train batch load time: {avg_batch_time:.3f}s')\n",
        "if avg_batch_time > 1.0:\n",
        "    print('Data loading is slow. Try NUM_WORKERS=0 (Windows), increase BATCH_SIZE, or move data to SSD.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "0c396168",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2026-02-03T18:45:42.121992Z",
          "start_time": "2026-02-03T18:45:41.951743Z"
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=12, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def build_model(num_classes, use_pretrained=False):\n",
        "    if use_pretrained:\n",
        "        weights = models.ResNet18_Weights.DEFAULT\n",
        "        model = models.resnet18(weights=weights)\n",
        "    else:\n",
        "        model = models.resnet18(weights=None)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, num_classes)\n",
        "    return model\n",
        "\n",
        "model = build_model(num_classes, USE_PRETRAINED).to(device)\n",
        "model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "781040a0",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2026-02-03T18:45:43.814255Z",
          "start_time": "2026-02-03T18:45:43.772071Z"
        }
      },
      "outputs": [],
      "source": [
        "# Fast class weights using targets (no image loading)\n",
        "def compute_class_weights_from_targets(dataset, num_classes):\n",
        "    if isinstance(dataset, Subset):\n",
        "        targets = np.array(dataset.dataset.targets)[dataset.indices]\n",
        "    else:\n",
        "        targets = np.array(dataset.targets)\n",
        "    counts = np.bincount(targets, minlength=num_classes)\n",
        "    counts = np.maximum(counts, 1)\n",
        "    weights = counts.sum() / counts\n",
        "    weights = weights / weights.mean()\n",
        "    return torch.tensor(weights, dtype=torch.float32)\n",
        "\n",
        "class_weights = compute_class_weights_from_targets(train_ds_sub, num_classes).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
        "scaler = torch.amp.GradScaler(enabled=torch.cuda.is_available(), device=\"cuda\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "46adaa15",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2026-02-03T18:45:44.814126Z",
          "start_time": "2026-02-03T18:45:44.804478Z"
        }
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(model, loader, criterion, optimizer, scaler, log_every=100):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    start_time = time.time()\n",
        "    for batch_idx, (images, labels) in enumerate(loader, start=1):\n",
        "        images = images.to(device, non_blocking=True)\n",
        "        labels = labels.to(device, non_blocking=True)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "        preds = outputs.argmax(dim=1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "        if batch_idx % log_every == 0:\n",
        "            elapsed = time.time() - start_time\n",
        "            avg_loss = running_loss / total\n",
        "            avg_acc = correct / total\n",
        "            print(f'  [batch {batch_idx}/{len(loader)}] loss {avg_loss:.4f} acc {avg_acc:.4f} | {elapsed:.1f}s elapsed')\n",
        "    return running_loss / total, correct / total\n",
        "\n",
        "def evaluate(model, loader, criterion=None):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    for images, labels in loader:\n",
        "        images = images.to(device, non_blocking=True)\n",
        "        labels = labels.to(device, non_blocking=True)\n",
        "        outputs = model(images)\n",
        "        if criterion is not None:\n",
        "            loss = criterion(outputs, labels)\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "        preds = outputs.argmax(dim=1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "        all_preds.append(preds.cpu())\n",
        "        all_labels.append(labels.cpu())\n",
        "    all_preds = torch.cat(all_preds)\n",
        "    all_labels = torch.cat(all_labels)\n",
        "    acc = correct / total if total else 0\n",
        "    loss_avg = (running_loss / total) if criterion is not None and total else None\n",
        "    return loss_avg, acc, all_preds, all_labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "faeebdd4",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2026-02-04T02:05:01.676725Z",
          "start_time": "2026-02-03T18:45:47.304942Z"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\willi\\AppData\\Local\\Temp\\ipykernel_32776\\4185219890.py:11: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  [batch 100/10989] loss 1.6961 acc 0.2153 | 73.6s elapsed\n",
            "  [batch 200/10989] loss 1.5946 acc 0.2472 | 147.1s elapsed\n",
            "  [batch 300/10989] loss 1.5099 acc 0.2839 | 219.6s elapsed\n",
            "  [batch 400/10989] loss 1.4426 acc 0.3105 | 293.3s elapsed\n",
            "  [batch 500/10989] loss 1.3937 acc 0.3351 | 364.5s elapsed\n",
            "  [batch 600/10989] loss 1.3516 acc 0.3538 | 435.1s elapsed\n",
            "  [batch 700/10989] loss 1.3174 acc 0.3686 | 506.0s elapsed\n",
            "  [batch 800/10989] loss 1.2905 acc 0.3819 | 575.6s elapsed\n",
            "  [batch 900/10989] loss 1.2625 acc 0.3948 | 644.8s elapsed\n",
            "  [batch 1000/10989] loss 1.2407 acc 0.4048 | 714.3s elapsed\n",
            "  [batch 1100/10989] loss 1.2171 acc 0.4151 | 783.1s elapsed\n",
            "  [batch 1200/10989] loss 1.1941 acc 0.4249 | 851.2s elapsed\n",
            "  [batch 1300/10989] loss 1.1764 acc 0.4334 | 920.0s elapsed\n",
            "  [batch 1400/10989] loss 1.1596 acc 0.4416 | 987.7s elapsed\n",
            "  [batch 1500/10989] loss 1.1430 acc 0.4488 | 1055.0s elapsed\n",
            "  [batch 1600/10989] loss 1.1264 acc 0.4562 | 1121.5s elapsed\n",
            "  [batch 1700/10989] loss 1.1141 acc 0.4618 | 1188.0s elapsed\n",
            "  [batch 1800/10989] loss 1.1003 acc 0.4678 | 1257.7s elapsed\n",
            "  [batch 1900/10989] loss 1.0861 acc 0.4743 | 1323.4s elapsed\n",
            "  [batch 2000/10989] loss 1.0730 acc 0.4802 | 1388.8s elapsed\n",
            "  [batch 2100/10989] loss 1.0594 acc 0.4862 | 1453.9s elapsed\n",
            "  [batch 2200/10989] loss 1.0470 acc 0.4921 | 1521.1s elapsed\n",
            "  [batch 2300/10989] loss 1.0364 acc 0.4976 | 1586.5s elapsed\n",
            "  [batch 2400/10989] loss 1.0268 acc 0.5021 | 1650.5s elapsed\n",
            "  [batch 2500/10989] loss 1.0173 acc 0.5066 | 1715.0s elapsed\n",
            "  [batch 2600/10989] loss 1.0074 acc 0.5112 | 1781.8s elapsed\n",
            "  [batch 2700/10989] loss 0.9988 acc 0.5155 | 1853.0s elapsed\n",
            "  [batch 2800/10989] loss 0.9895 acc 0.5201 | 1917.1s elapsed\n",
            "  [batch 2900/10989] loss 0.9803 acc 0.5247 | 1984.6s elapsed\n",
            "  [batch 3000/10989] loss 0.9712 acc 0.5291 | 2057.2s elapsed\n",
            "  [batch 3100/10989] loss 0.9619 acc 0.5333 | 2129.6s elapsed\n",
            "  [batch 3200/10989] loss 0.9546 acc 0.5366 | 2201.0s elapsed\n",
            "  [batch 3300/10989] loss 0.9466 acc 0.5405 | 2271.5s elapsed\n",
            "  [batch 3400/10989] loss 0.9391 acc 0.5440 | 2344.1s elapsed\n",
            "  [batch 3500/10989] loss 0.9308 acc 0.5478 | 2414.2s elapsed\n",
            "  [batch 3600/10989] loss 0.9237 acc 0.5511 | 2484.2s elapsed\n",
            "  [batch 3700/10989] loss 0.9161 acc 0.5547 | 2553.6s elapsed\n",
            "  [batch 3800/10989] loss 0.9091 acc 0.5580 | 2622.9s elapsed\n",
            "  [batch 3900/10989] loss 0.9029 acc 0.5609 | 2692.8s elapsed\n",
            "  [batch 4000/10989] loss 0.8962 acc 0.5638 | 2763.6s elapsed\n",
            "  [batch 4100/10989] loss 0.8900 acc 0.5668 | 2835.5s elapsed\n",
            "  [batch 4200/10989] loss 0.8840 acc 0.5698 | 2906.6s elapsed\n",
            "  [batch 4300/10989] loss 0.8778 acc 0.5727 | 2976.5s elapsed\n",
            "  [batch 4400/10989] loss 0.8723 acc 0.5755 | 3044.4s elapsed\n",
            "  [batch 4500/10989] loss 0.8666 acc 0.5781 | 3111.6s elapsed\n",
            "  [batch 4600/10989] loss 0.8607 acc 0.5809 | 3179.1s elapsed\n",
            "  [batch 4700/10989] loss 0.8557 acc 0.5834 | 3245.7s elapsed\n",
            "  [batch 4800/10989] loss 0.8503 acc 0.5861 | 3314.4s elapsed\n",
            "  [batch 4900/10989] loss 0.8450 acc 0.5886 | 3385.2s elapsed\n",
            "  [batch 5000/10989] loss 0.8397 acc 0.5911 | 3455.0s elapsed\n",
            "  [batch 5100/10989] loss 0.8351 acc 0.5934 | 3524.7s elapsed\n",
            "  [batch 5200/10989] loss 0.8299 acc 0.5959 | 3594.0s elapsed\n",
            "  [batch 5300/10989] loss 0.8250 acc 0.5982 | 3664.0s elapsed\n",
            "  [batch 5400/10989] loss 0.8202 acc 0.6006 | 3733.6s elapsed\n",
            "  [batch 5500/10989] loss 0.8158 acc 0.6029 | 3802.4s elapsed\n",
            "  [batch 5600/10989] loss 0.8109 acc 0.6052 | 3871.2s elapsed\n",
            "  [batch 5700/10989] loss 0.8061 acc 0.6075 | 3938.6s elapsed\n",
            "  [batch 5800/10989] loss 0.8017 acc 0.6097 | 4006.0s elapsed\n",
            "  [batch 5900/10989] loss 0.7972 acc 0.6118 | 4073.0s elapsed\n",
            "  [batch 6000/10989] loss 0.7932 acc 0.6138 | 4140.9s elapsed\n",
            "  [batch 6100/10989] loss 0.7888 acc 0.6159 | 4208.9s elapsed\n",
            "  [batch 6200/10989] loss 0.7848 acc 0.6178 | 4275.6s elapsed\n",
            "  [batch 6300/10989] loss 0.7806 acc 0.6197 | 4342.8s elapsed\n",
            "  [batch 6400/10989] loss 0.7759 acc 0.6218 | 4409.7s elapsed\n",
            "  [batch 6500/10989] loss 0.7722 acc 0.6236 | 4476.8s elapsed\n",
            "  [batch 6600/10989] loss 0.7690 acc 0.6251 | 4539.8s elapsed\n",
            "  [batch 6700/10989] loss 0.7650 acc 0.6270 | 4605.4s elapsed\n",
            "  [batch 6800/10989] loss 0.7611 acc 0.6289 | 4673.6s elapsed\n",
            "  [batch 6900/10989] loss 0.7577 acc 0.6306 | 4736.9s elapsed\n",
            "  [batch 7000/10989] loss 0.7541 acc 0.6323 | 4800.4s elapsed\n",
            "  [batch 7100/10989] loss 0.7502 acc 0.6341 | 4863.2s elapsed\n",
            "  [batch 7200/10989] loss 0.7470 acc 0.6356 | 4925.8s elapsed\n",
            "  [batch 7300/10989] loss 0.7436 acc 0.6372 | 4988.3s elapsed\n",
            "  [batch 7400/10989] loss 0.7400 acc 0.6388 | 5053.4s elapsed\n",
            "  [batch 7500/10989] loss 0.7367 acc 0.6404 | 5115.9s elapsed\n",
            "  [batch 7600/10989] loss 0.7335 acc 0.6420 | 5179.3s elapsed\n",
            "  [batch 7700/10989] loss 0.7301 acc 0.6434 | 5242.7s elapsed\n",
            "  [batch 7800/10989] loss 0.7269 acc 0.6448 | 5304.8s elapsed\n",
            "  [batch 7900/10989] loss 0.7236 acc 0.6463 | 5367.3s elapsed\n",
            "  [batch 8000/10989] loss 0.7208 acc 0.6477 | 5430.0s elapsed\n",
            "  [batch 8100/10989] loss 0.7177 acc 0.6491 | 5495.2s elapsed\n",
            "  [batch 8200/10989] loss 0.7147 acc 0.6505 | 5559.5s elapsed\n",
            "  [batch 8300/10989] loss 0.7119 acc 0.6518 | 5622.4s elapsed\n",
            "  [batch 8400/10989] loss 0.7092 acc 0.6530 | 5684.0s elapsed\n",
            "  [batch 8500/10989] loss 0.7064 acc 0.6542 | 5745.1s elapsed\n",
            "  [batch 8600/10989] loss 0.7037 acc 0.6555 | 5808.8s elapsed\n",
            "  [batch 8700/10989] loss 0.7008 acc 0.6568 | 5870.3s elapsed\n",
            "  [batch 8800/10989] loss 0.6983 acc 0.6580 | 5931.5s elapsed\n",
            "  [batch 8900/10989] loss 0.6959 acc 0.6592 | 5992.5s elapsed\n",
            "  [batch 9000/10989] loss 0.6933 acc 0.6605 | 6053.2s elapsed\n",
            "  [batch 9100/10989] loss 0.6905 acc 0.6618 | 6114.0s elapsed\n",
            "  [batch 9200/10989] loss 0.6878 acc 0.6630 | 6174.4s elapsed\n",
            "  [batch 9300/10989] loss 0.6852 acc 0.6642 | 6234.9s elapsed\n",
            "  [batch 9400/10989] loss 0.6828 acc 0.6652 | 6294.9s elapsed\n",
            "  [batch 9500/10989] loss 0.6803 acc 0.6665 | 6355.4s elapsed\n",
            "  [batch 9600/10989] loss 0.6778 acc 0.6676 | 6416.3s elapsed\n",
            "  [batch 9700/10989] loss 0.6756 acc 0.6687 | 6477.2s elapsed\n",
            "  [batch 9800/10989] loss 0.6732 acc 0.6698 | 6537.3s elapsed\n",
            "  [batch 9900/10989] loss 0.6710 acc 0.6709 | 6597.1s elapsed\n",
            "  [batch 10000/10989] loss 0.6683 acc 0.6721 | 6656.2s elapsed\n",
            "  [batch 10100/10989] loss 0.6656 acc 0.6732 | 6715.4s elapsed\n",
            "  [batch 10200/10989] loss 0.6634 acc 0.6743 | 6774.9s elapsed\n",
            "  [batch 10300/10989] loss 0.6611 acc 0.6754 | 6834.2s elapsed\n",
            "  [batch 10400/10989] loss 0.6590 acc 0.6763 | 6892.9s elapsed\n",
            "  [batch 10500/10989] loss 0.6568 acc 0.6774 | 6953.4s elapsed\n",
            "  [batch 10600/10989] loss 0.6544 acc 0.6784 | 7011.5s elapsed\n",
            "  [batch 10700/10989] loss 0.6520 acc 0.6795 | 7069.7s elapsed\n",
            "  [batch 10800/10989] loss 0.6499 acc 0.6806 | 7127.6s elapsed\n",
            "  [batch 10900/10989] loss 0.6478 acc 0.6815 | 7187.9s elapsed\n",
            "Epoch 1/3 | train loss 0.6459 acc 0.6824 | val loss 0.6470 acc 0.7391\n",
            "  [batch 100/10989] loss 0.4554 acc 0.7794 | 60.5s elapsed\n",
            "  [batch 200/10989] loss 0.4278 acc 0.7909 | 121.0s elapsed\n",
            "  [batch 300/10989] loss 0.4165 acc 0.7945 | 182.4s elapsed\n",
            "  [batch 400/10989] loss 0.4073 acc 0.7976 | 242.7s elapsed\n",
            "  [batch 500/10989] loss 0.4049 acc 0.7980 | 302.4s elapsed\n",
            "  [batch 600/10989] loss 0.4068 acc 0.7974 | 362.5s elapsed\n",
            "  [batch 700/10989] loss 0.4029 acc 0.7979 | 422.3s elapsed\n",
            "  [batch 800/10989] loss 0.4056 acc 0.7977 | 482.7s elapsed\n",
            "  [batch 900/10989] loss 0.4053 acc 0.7972 | 542.7s elapsed\n",
            "  [batch 1000/10989] loss 0.4059 acc 0.7973 | 603.1s elapsed\n",
            "  [batch 1100/10989] loss 0.4032 acc 0.7980 | 663.0s elapsed\n",
            "  [batch 1200/10989] loss 0.4025 acc 0.7986 | 723.0s elapsed\n",
            "  [batch 1300/10989] loss 0.4030 acc 0.7987 | 782.5s elapsed\n",
            "  [batch 1400/10989] loss 0.4044 acc 0.7981 | 842.0s elapsed\n",
            "  [batch 1500/10989] loss 0.4035 acc 0.7978 | 900.9s elapsed\n",
            "  [batch 1600/10989] loss 0.4032 acc 0.7981 | 960.3s elapsed\n",
            "  [batch 1700/10989] loss 0.4020 acc 0.7986 | 1020.0s elapsed\n",
            "  [batch 1800/10989] loss 0.4013 acc 0.7987 | 1079.5s elapsed\n",
            "  [batch 1900/10989] loss 0.3998 acc 0.7993 | 1138.9s elapsed\n",
            "  [batch 2000/10989] loss 0.3968 acc 0.8007 | 1198.0s elapsed\n",
            "  [batch 2100/10989] loss 0.3970 acc 0.8008 | 1257.1s elapsed\n",
            "  [batch 2200/10989] loss 0.3964 acc 0.8009 | 1316.5s elapsed\n",
            "  [batch 2300/10989] loss 0.3976 acc 0.8005 | 1375.4s elapsed\n",
            "  [batch 2400/10989] loss 0.3966 acc 0.8010 | 1434.6s elapsed\n",
            "  [batch 2500/10989] loss 0.3942 acc 0.8018 | 1494.6s elapsed\n",
            "  [batch 2600/10989] loss 0.3928 acc 0.8019 | 1552.7s elapsed\n",
            "  [batch 2700/10989] loss 0.3929 acc 0.8016 | 1611.8s elapsed\n",
            "  [batch 2800/10989] loss 0.3920 acc 0.8016 | 1670.6s elapsed\n",
            "  [batch 2900/10989] loss 0.3920 acc 0.8015 | 1728.7s elapsed\n",
            "  [batch 3000/10989] loss 0.3910 acc 0.8019 | 1786.6s elapsed\n",
            "  [batch 3100/10989] loss 0.3898 acc 0.8021 | 1844.1s elapsed\n",
            "  [batch 3200/10989] loss 0.3891 acc 0.8024 | 1902.0s elapsed\n",
            "  [batch 3300/10989] loss 0.3881 acc 0.8026 | 1959.9s elapsed\n",
            "  [batch 3400/10989] loss 0.3877 acc 0.8028 | 2018.3s elapsed\n",
            "  [batch 3500/10989] loss 0.3871 acc 0.8031 | 2075.8s elapsed\n",
            "  [batch 3600/10989] loss 0.3865 acc 0.8033 | 2133.9s elapsed\n",
            "  [batch 3700/10989] loss 0.3856 acc 0.8036 | 2191.7s elapsed\n",
            "  [batch 3800/10989] loss 0.3853 acc 0.8038 | 2249.0s elapsed\n",
            "  [batch 3900/10989] loss 0.3851 acc 0.8039 | 2306.1s elapsed\n",
            "  [batch 4000/10989] loss 0.3847 acc 0.8040 | 2363.5s elapsed\n",
            "  [batch 4100/10989] loss 0.3836 acc 0.8044 | 2420.0s elapsed\n",
            "  [batch 4200/10989] loss 0.3823 acc 0.8050 | 2476.6s elapsed\n",
            "  [batch 4300/10989] loss 0.3820 acc 0.8052 | 2533.3s elapsed\n",
            "  [batch 4400/10989] loss 0.3815 acc 0.8054 | 2589.9s elapsed\n",
            "  [batch 4500/10989] loss 0.3813 acc 0.8056 | 2646.1s elapsed\n",
            "  [batch 4600/10989] loss 0.3804 acc 0.8060 | 2702.5s elapsed\n",
            "  [batch 4700/10989] loss 0.3794 acc 0.8064 | 2758.1s elapsed\n",
            "  [batch 4800/10989] loss 0.3788 acc 0.8068 | 2814.3s elapsed\n",
            "  [batch 4900/10989] loss 0.3780 acc 0.8071 | 2871.2s elapsed\n",
            "  [batch 5000/10989] loss 0.3767 acc 0.8076 | 2930.1s elapsed\n",
            "  [batch 5100/10989] loss 0.3763 acc 0.8077 | 2986.5s elapsed\n",
            "  [batch 5200/10989] loss 0.3757 acc 0.8079 | 3043.0s elapsed\n",
            "  [batch 5300/10989] loss 0.3751 acc 0.8081 | 3099.0s elapsed\n",
            "  [batch 5400/10989] loss 0.3744 acc 0.8083 | 3155.8s elapsed\n",
            "  [batch 5500/10989] loss 0.3739 acc 0.8085 | 3211.9s elapsed\n",
            "  [batch 5600/10989] loss 0.3733 acc 0.8088 | 3268.5s elapsed\n",
            "  [batch 5700/10989] loss 0.3727 acc 0.8090 | 3324.4s elapsed\n",
            "  [batch 5800/10989] loss 0.3722 acc 0.8093 | 3380.0s elapsed\n",
            "  [batch 5900/10989] loss 0.3716 acc 0.8096 | 3436.5s elapsed\n",
            "  [batch 6000/10989] loss 0.3705 acc 0.8101 | 3492.3s elapsed\n",
            "  [batch 6100/10989] loss 0.3695 acc 0.8104 | 3547.8s elapsed\n",
            "  [batch 6200/10989] loss 0.3686 acc 0.8108 | 3603.7s elapsed\n",
            "  [batch 6300/10989] loss 0.3681 acc 0.8110 | 3658.9s elapsed\n",
            "  [batch 6400/10989] loss 0.3673 acc 0.8113 | 3714.1s elapsed\n",
            "  [batch 6500/10989] loss 0.3667 acc 0.8116 | 3769.2s elapsed\n",
            "  [batch 6600/10989] loss 0.3663 acc 0.8118 | 3823.8s elapsed\n",
            "  [batch 6700/10989] loss 0.3654 acc 0.8122 | 3878.4s elapsed\n",
            "  [batch 6800/10989] loss 0.3650 acc 0.8123 | 3933.3s elapsed\n",
            "  [batch 6900/10989] loss 0.3644 acc 0.8125 | 3988.3s elapsed\n",
            "  [batch 7000/10989] loss 0.3639 acc 0.8126 | 4043.5s elapsed\n",
            "  [batch 7100/10989] loss 0.3630 acc 0.8129 | 4096.1s elapsed\n",
            "  [batch 7200/10989] loss 0.3625 acc 0.8132 | 4147.8s elapsed\n",
            "  [batch 7300/10989] loss 0.3619 acc 0.8135 | 4199.5s elapsed\n",
            "  [batch 7400/10989] loss 0.3613 acc 0.8138 | 4251.6s elapsed\n",
            "  [batch 7500/10989] loss 0.3604 acc 0.8142 | 4302.5s elapsed\n",
            "  [batch 7600/10989] loss 0.3599 acc 0.8144 | 4354.5s elapsed\n",
            "  [batch 7700/10989] loss 0.3590 acc 0.8148 | 4406.5s elapsed\n",
            "  [batch 7800/10989] loss 0.3583 acc 0.8149 | 4457.8s elapsed\n",
            "  [batch 7900/10989] loss 0.3579 acc 0.8151 | 4512.1s elapsed\n",
            "  [batch 8000/10989] loss 0.3571 acc 0.8154 | 4563.0s elapsed\n",
            "  [batch 8100/10989] loss 0.3564 acc 0.8156 | 4614.5s elapsed\n",
            "  [batch 8200/10989] loss 0.3559 acc 0.8158 | 4666.0s elapsed\n",
            "  [batch 8300/10989] loss 0.3554 acc 0.8160 | 4717.1s elapsed\n",
            "  [batch 8400/10989] loss 0.3550 acc 0.8162 | 4774.7s elapsed\n",
            "  [batch 8500/10989] loss 0.3546 acc 0.8163 | 4831.5s elapsed\n",
            "  [batch 8600/10989] loss 0.3537 acc 0.8167 | 4885.4s elapsed\n",
            "  [batch 8700/10989] loss 0.3533 acc 0.8168 | 4939.7s elapsed\n",
            "  [batch 8800/10989] loss 0.3528 acc 0.8170 | 4995.0s elapsed\n",
            "  [batch 8900/10989] loss 0.3528 acc 0.8170 | 5049.5s elapsed\n",
            "  [batch 9000/10989] loss 0.3523 acc 0.8172 | 5103.9s elapsed\n",
            "  [batch 9100/10989] loss 0.3519 acc 0.8173 | 5159.2s elapsed\n",
            "  [batch 9200/10989] loss 0.3513 acc 0.8176 | 5214.0s elapsed\n",
            "  [batch 9300/10989] loss 0.3510 acc 0.8178 | 5267.8s elapsed\n",
            "  [batch 9400/10989] loss 0.3507 acc 0.8179 | 5320.7s elapsed\n",
            "  [batch 9500/10989] loss 0.3501 acc 0.8181 | 5373.2s elapsed\n",
            "  [batch 9600/10989] loss 0.3496 acc 0.8183 | 5427.4s elapsed\n",
            "  [batch 9700/10989] loss 0.3490 acc 0.8186 | 5482.0s elapsed\n",
            "  [batch 9800/10989] loss 0.3483 acc 0.8188 | 5535.0s elapsed\n",
            "  [batch 9900/10989] loss 0.3478 acc 0.8189 | 5587.2s elapsed\n",
            "  [batch 10000/10989] loss 0.3474 acc 0.8191 | 5639.4s elapsed\n",
            "  [batch 10100/10989] loss 0.3466 acc 0.8194 | 5692.0s elapsed\n",
            "  [batch 10200/10989] loss 0.3461 acc 0.8197 | 5746.0s elapsed\n",
            "  [batch 10300/10989] loss 0.3456 acc 0.8199 | 5799.5s elapsed\n",
            "  [batch 10400/10989] loss 0.3449 acc 0.8202 | 5853.9s elapsed\n",
            "  [batch 10500/10989] loss 0.3444 acc 0.8204 | 5922.6s elapsed\n",
            "  [batch 10600/10989] loss 0.3440 acc 0.8206 | 5974.8s elapsed\n",
            "  [batch 10700/10989] loss 0.3434 acc 0.8208 | 6049.5s elapsed\n",
            "  [batch 10800/10989] loss 0.3429 acc 0.8210 | 6103.7s elapsed\n",
            "  [batch 10900/10989] loss 0.3422 acc 0.8213 | 6158.1s elapsed\n",
            "Epoch 2/3 | train loss 0.3418 acc 0.8215 | val loss 0.4839 acc 0.7960\n",
            "  [batch 100/10989] loss 0.2902 acc 0.8462 | 55.0s elapsed\n",
            "  [batch 200/10989] loss 0.2761 acc 0.8488 | 107.5s elapsed\n",
            "  [batch 300/10989] loss 0.2783 acc 0.8487 | 162.4s elapsed\n",
            "  [batch 400/10989] loss 0.2827 acc 0.8491 | 228.9s elapsed\n",
            "  [batch 500/10989] loss 0.2839 acc 0.8482 | 285.2s elapsed\n",
            "  [batch 600/10989] loss 0.2814 acc 0.8480 | 336.8s elapsed\n",
            "  [batch 700/10989] loss 0.2837 acc 0.8485 | 389.0s elapsed\n",
            "  [batch 800/10989] loss 0.2845 acc 0.8483 | 442.3s elapsed\n",
            "  [batch 900/10989] loss 0.2832 acc 0.8488 | 496.1s elapsed\n",
            "  [batch 1000/10989] loss 0.2808 acc 0.8492 | 552.3s elapsed\n",
            "  [batch 1100/10989] loss 0.2790 acc 0.8495 | 607.3s elapsed\n",
            "  [batch 1200/10989] loss 0.2794 acc 0.8492 | 664.3s elapsed\n",
            "  [batch 1300/10989] loss 0.2802 acc 0.8489 | 720.8s elapsed\n",
            "  [batch 1400/10989] loss 0.2804 acc 0.8492 | 777.2s elapsed\n",
            "  [batch 1500/10989] loss 0.2790 acc 0.8492 | 835.0s elapsed\n",
            "  [batch 1600/10989] loss 0.2793 acc 0.8490 | 889.9s elapsed\n",
            "  [batch 1700/10989] loss 0.2798 acc 0.8492 | 935.6s elapsed\n",
            "  [batch 1800/10989] loss 0.2785 acc 0.8496 | 980.3s elapsed\n",
            "  [batch 1900/10989] loss 0.2770 acc 0.8501 | 1024.5s elapsed\n",
            "  [batch 2000/10989] loss 0.2779 acc 0.8500 | 1079.8s elapsed\n",
            "  [batch 2100/10989] loss 0.2792 acc 0.8495 | 1128.8s elapsed\n",
            "  [batch 2200/10989] loss 0.2787 acc 0.8495 | 1177.5s elapsed\n",
            "  [batch 2300/10989] loss 0.2776 acc 0.8497 | 1225.8s elapsed\n",
            "  [batch 2400/10989] loss 0.2776 acc 0.8497 | 1274.2s elapsed\n",
            "  [batch 2500/10989] loss 0.2769 acc 0.8499 | 1322.3s elapsed\n",
            "  [batch 2600/10989] loss 0.2766 acc 0.8501 | 1372.2s elapsed\n",
            "  [batch 2700/10989] loss 0.2753 acc 0.8504 | 1421.0s elapsed\n",
            "  [batch 2800/10989] loss 0.2746 acc 0.8504 | 1469.3s elapsed\n",
            "  [batch 2900/10989] loss 0.2745 acc 0.8504 | 1517.7s elapsed\n",
            "  [batch 3000/10989] loss 0.2739 acc 0.8506 | 1565.8s elapsed\n",
            "  [batch 3100/10989] loss 0.2738 acc 0.8507 | 1614.4s elapsed\n",
            "  [batch 3200/10989] loss 0.2752 acc 0.8503 | 1663.1s elapsed\n",
            "  [batch 3300/10989] loss 0.2747 acc 0.8505 | 1713.1s elapsed\n",
            "  [batch 3400/10989] loss 0.2741 acc 0.8505 | 1762.6s elapsed\n",
            "  [batch 3500/10989] loss 0.2741 acc 0.8506 | 1810.5s elapsed\n",
            "  [batch 3600/10989] loss 0.2736 acc 0.8510 | 1858.3s elapsed\n",
            "  [batch 3700/10989] loss 0.2731 acc 0.8511 | 1906.4s elapsed\n",
            "  [batch 3800/10989] loss 0.2724 acc 0.8513 | 1963.8s elapsed\n",
            "  [batch 3900/10989] loss 0.2719 acc 0.8515 | 2012.5s elapsed\n",
            "  [batch 4000/10989] loss 0.2713 acc 0.8518 | 2061.0s elapsed\n",
            "  [batch 4100/10989] loss 0.2711 acc 0.8519 | 2109.7s elapsed\n",
            "  [batch 4200/10989] loss 0.2707 acc 0.8521 | 2160.0s elapsed\n",
            "  [batch 4300/10989] loss 0.2708 acc 0.8522 | 2216.8s elapsed\n",
            "  [batch 4400/10989] loss 0.2704 acc 0.8522 | 2271.7s elapsed\n",
            "  [batch 4500/10989] loss 0.2697 acc 0.8524 | 2324.8s elapsed\n",
            "  [batch 4600/10989] loss 0.2695 acc 0.8524 | 2377.9s elapsed\n",
            "  [batch 4700/10989] loss 0.2690 acc 0.8526 | 2430.9s elapsed\n",
            "  [batch 4800/10989] loss 0.2684 acc 0.8529 | 2482.3s elapsed\n",
            "  [batch 4900/10989] loss 0.2684 acc 0.8529 | 2530.0s elapsed\n",
            "  [batch 5000/10989] loss 0.2680 acc 0.8530 | 2577.8s elapsed\n",
            "  [batch 5100/10989] loss 0.2677 acc 0.8532 | 2625.5s elapsed\n",
            "  [batch 5200/10989] loss 0.2675 acc 0.8533 | 2672.1s elapsed\n",
            "  [batch 5300/10989] loss 0.2669 acc 0.8535 | 2721.1s elapsed\n",
            "  [batch 5400/10989] loss 0.2665 acc 0.8537 | 2768.6s elapsed\n",
            "  [batch 5500/10989] loss 0.2661 acc 0.8537 | 2818.6s elapsed\n",
            "  [batch 5600/10989] loss 0.2656 acc 0.8539 | 2867.8s elapsed\n",
            "  [batch 5700/10989] loss 0.2655 acc 0.8539 | 2917.3s elapsed\n",
            "  [batch 5800/10989] loss 0.2651 acc 0.8541 | 2966.1s elapsed\n",
            "  [batch 5900/10989] loss 0.2647 acc 0.8541 | 3014.6s elapsed\n",
            "  [batch 6000/10989] loss 0.2645 acc 0.8541 | 3062.4s elapsed\n",
            "  [batch 6100/10989] loss 0.2647 acc 0.8541 | 3110.5s elapsed\n",
            "  [batch 6200/10989] loss 0.2648 acc 0.8543 | 3158.8s elapsed\n",
            "  [batch 6300/10989] loss 0.2645 acc 0.8543 | 3206.8s elapsed\n",
            "  [batch 6400/10989] loss 0.2640 acc 0.8545 | 3254.4s elapsed\n",
            "  [batch 6500/10989] loss 0.2636 acc 0.8547 | 3302.9s elapsed\n",
            "  [batch 6600/10989] loss 0.2632 acc 0.8548 | 3350.7s elapsed\n",
            "  [batch 6700/10989] loss 0.2627 acc 0.8550 | 3398.4s elapsed\n",
            "  [batch 6800/10989] loss 0.2625 acc 0.8550 | 3446.4s elapsed\n",
            "  [batch 6900/10989] loss 0.2625 acc 0.8551 | 3494.2s elapsed\n",
            "  [batch 7000/10989] loss 0.2620 acc 0.8552 | 3541.7s elapsed\n",
            "  [batch 7100/10989] loss 0.2615 acc 0.8554 | 3589.5s elapsed\n",
            "  [batch 7200/10989] loss 0.2611 acc 0.8555 | 3637.2s elapsed\n",
            "  [batch 7300/10989] loss 0.2608 acc 0.8555 | 3684.9s elapsed\n",
            "  [batch 7400/10989] loss 0.2607 acc 0.8556 | 3732.5s elapsed\n",
            "  [batch 7500/10989] loss 0.2604 acc 0.8557 | 3780.2s elapsed\n",
            "  [batch 7600/10989] loss 0.2604 acc 0.8558 | 3827.7s elapsed\n",
            "  [batch 7700/10989] loss 0.2600 acc 0.8560 | 3875.7s elapsed\n",
            "  [batch 7800/10989] loss 0.2598 acc 0.8561 | 3923.1s elapsed\n",
            "  [batch 7900/10989] loss 0.2593 acc 0.8564 | 3970.6s elapsed\n",
            "  [batch 8000/10989] loss 0.2591 acc 0.8564 | 4017.8s elapsed\n",
            "  [batch 8100/10989] loss 0.2591 acc 0.8565 | 4065.3s elapsed\n",
            "  [batch 8200/10989] loss 0.2588 acc 0.8566 | 4112.5s elapsed\n",
            "  [batch 8300/10989] loss 0.2586 acc 0.8567 | 4159.4s elapsed\n",
            "  [batch 8400/10989] loss 0.2580 acc 0.8569 | 4206.7s elapsed\n",
            "  [batch 8500/10989] loss 0.2578 acc 0.8570 | 4253.9s elapsed\n",
            "  [batch 8600/10989] loss 0.2575 acc 0.8572 | 4301.1s elapsed\n",
            "  [batch 8700/10989] loss 0.2573 acc 0.8573 | 4348.5s elapsed\n",
            "  [batch 8800/10989] loss 0.2572 acc 0.8573 | 4395.7s elapsed\n",
            "  [batch 8900/10989] loss 0.2570 acc 0.8574 | 4443.0s elapsed\n",
            "  [batch 9000/10989] loss 0.2565 acc 0.8577 | 4490.4s elapsed\n",
            "  [batch 9100/10989] loss 0.2561 acc 0.8578 | 4537.1s elapsed\n",
            "  [batch 9200/10989] loss 0.2560 acc 0.8579 | 4583.8s elapsed\n",
            "  [batch 9300/10989] loss 0.2558 acc 0.8580 | 4630.3s elapsed\n",
            "  [batch 9400/10989] loss 0.2555 acc 0.8581 | 4677.0s elapsed\n",
            "  [batch 9500/10989] loss 0.2551 acc 0.8582 | 4723.4s elapsed\n",
            "  [batch 9600/10989] loss 0.2546 acc 0.8584 | 4770.0s elapsed\n",
            "  [batch 9700/10989] loss 0.2542 acc 0.8585 | 4816.7s elapsed\n",
            "  [batch 9800/10989] loss 0.2539 acc 0.8586 | 4863.1s elapsed\n",
            "  [batch 9900/10989] loss 0.2537 acc 0.8587 | 4909.6s elapsed\n",
            "  [batch 10000/10989] loss 0.2535 acc 0.8589 | 4956.0s elapsed\n",
            "  [batch 10100/10989] loss 0.2531 acc 0.8590 | 5002.4s elapsed\n",
            "  [batch 10200/10989] loss 0.2530 acc 0.8591 | 5048.8s elapsed\n",
            "  [batch 10300/10989] loss 0.2526 acc 0.8593 | 5094.9s elapsed\n",
            "  [batch 10400/10989] loss 0.2521 acc 0.8594 | 5141.3s elapsed\n",
            "  [batch 10500/10989] loss 0.2517 acc 0.8595 | 5187.7s elapsed\n",
            "  [batch 10600/10989] loss 0.2515 acc 0.8596 | 5234.4s elapsed\n",
            "  [batch 10700/10989] loss 0.2513 acc 0.8597 | 5280.5s elapsed\n",
            "  [batch 10800/10989] loss 0.2507 acc 0.8599 | 5327.6s elapsed\n",
            "  [batch 10900/10989] loss 0.2506 acc 0.8600 | 5373.9s elapsed\n",
            "Epoch 3/3 | train loss 0.2503 acc 0.8601 | val loss 0.4146 acc 0.8287\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.8287077826725404"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "best_val_acc = 0.0\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, scaler)\n",
        "    val_loss, val_acc, _, _ = evaluate(model, val_loader, criterion)\n",
        "    print(f'Epoch {epoch}/{EPOCHS} | train loss {train_loss:.4f} acc {train_acc:.4f} | val loss {val_loss:.4f} acc {val_acc:.4f}')\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        torch.save({'model_state': model.state_dict(), 'class_to_idx': class_to_idx}, 'rscd_resnet18_v2.pt')\n",
        "\n",
        "best_val_acc\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "ca08b100",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2026-02-04T06:22:23.662262Z",
          "start_time": "2026-02-04T06:13:41.185927Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val acc: 0.8287 | Test acc: 0.8397\n"
          ]
        }
      ],
      "source": [
        "val_loss, val_acc, val_preds, val_labels = evaluate(model, val_loader, criterion)\n",
        "test_loss, test_acc, test_preds, test_labels = evaluate(model, test_loader, criterion)\n",
        "print(f'Val acc: {val_acc:.4f} | Test acc: {test_acc:.4f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "cd9d9290453735e0",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2026-02-04T07:30:08.563239Z",
          "start_time": "2026-02-04T07:30:07.925084Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "               precision    recall  f1-score   support\n",
            "\n",
            "  dry_asphalt     0.8761    0.8195    0.8469      2460\n",
            "   dry_gravel     0.8206    0.8927    0.8551       820\n",
            "      dry_mud     0.8570    0.8622    0.8596       820\n",
            "   fresh_snow     0.9651    0.9793    0.9722       820\n",
            "          ice     0.8800    0.9390    0.9086       820\n",
            "  melted_snow     0.9452    0.9671    0.9560       820\n",
            "water_asphalt     0.8981    0.5968    0.7171      1890\n",
            " water_gravel     0.5924    0.9463    0.7286       820\n",
            "    water_mud     0.7009    0.9061    0.7904       820\n",
            "  wet_asphalt     0.8151    0.7630    0.7882      1890\n",
            "   wet_gravel     0.8361    0.8463    0.8412       820\n",
            "      wet_mud     0.8219    0.8329    0.8274       820\n",
            "\n",
            "     accuracy                         0.8287     13620\n",
            "    macro avg     0.8341    0.8626    0.8409     13620\n",
            " weighted avg     0.8427    0.8287    0.8278     13620\n",
            "\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x800 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "report = classification_report(val_labels, val_preds, target_names=class_names, digits=4)\n",
        "print(report)\n",
        "cm = confusion_matrix(val_labels, val_preds)\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, cmap='Blues', cbar=False)\n",
        "plt.title('Validation Confusion Matrix')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "1ad53a06",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2026-02-04T13:10:57.021084Z",
          "start_time": "2026-02-04T13:10:56.941113Z"
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('data/RSCD dataset-1million/test_50k/202202081538546-fresh_snow.jpg',\n",
              " [('fresh_snow', 0.9808152914047241),\n",
              "  ('ice', 0.01572992466390133),\n",
              "  ('melted_snow', 0.0033535591792315245),\n",
              "  ('water_asphalt', 8.611679368186742e-05),\n",
              "  ('water_mud', 1.2657484148803633e-05),\n",
              "  ('dry_asphalt', 1.1104524446636788e-06),\n",
              "  ('dry_gravel', 1.047055661729246e-06),\n",
              "  ('wet_mud', 1.422536968220811e-07),\n",
              "  ('wet_asphalt', 1.2291557993648894e-07),\n",
              "  ('dry_mud', 6.224284732070373e-08),\n",
              "  ('water_gravel', 2.454994074696515e-08),\n",
              "  ('wet_gravel', 1.0154735941014792e-09)])"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "@torch.no_grad()\n",
        "def predict_one(path: Path):\n",
        "    img = datasets.folder.default_loader(path)\n",
        "    x = eval_tfms(img).unsqueeze(0).to(device)\n",
        "    logits = model(x)\n",
        "    probs = torch.softmax(logits, dim=1).squeeze(0)\n",
        "    topk = torch.topk(probs, k=num_classes)\n",
        "    top_labels = [idx_to_class[i.item()] for i in topk.indices]\n",
        "    return list(zip(top_labels, topk.values.cpu().tolist()))\n",
        "\n",
        "# Example\n",
        "sample_path = \"data/RSCD dataset-1million/test_50k/202202081538546-fresh_snow.jpg\"\n",
        "sample_path, predict_one(sample_path)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "eit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "3.13.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
