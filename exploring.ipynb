{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb6f68fa",
   "metadata": {},
   "source": [
    "# RSCD Road Surface Classification\n",
    "\n",
    "This notebook trains an image classifier to predict road-surface classes using the RSCD dataset.\n",
    "It uses the train split (folders per class) and parses labels from filenames for validation/test.\n",
    "\n",
    "Notes:\n",
    "- The full dataset is large; use the `MAX_*_SAMPLES` knobs for quick experiments.\n",
    "- Pretrained weights require an existing local cache or internet access; set `USE_PRETRAINED` accordingly.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "3eafb6e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-03T18:45:18.076696Z",
     "start_time": "2026-02-03T18:45:18.066103Z"
    }
   },
   "source": [
    "from pathlib import Path\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torchvision import datasets, transforms, models\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "DATA_ROOT = Path('data') / 'RSCD dataset-1million'\n",
    "TRAIN_DIR = DATA_ROOT / 'train'\n",
    "VAL_DIR = DATA_ROOT / 'vali_20k'\n",
    "TEST_DIR = DATA_ROOT / 'test_50k'\n",
    "\n",
    "SEED = 42\n",
    "BATCH_SIZE = 64\n",
    "IMAGE_SIZE = 224\n",
    "NUM_WORKERS = 0\n",
    "EPOCHS = 3\n",
    "LR = 3e-4\n",
    "WEIGHT_DECAY = 1e-4\n",
    "USE_PRETRAINED = False\n",
    "\n",
    "MAX_TRAIN_SAMPLES = None  # e.g. 50000 for quick run\n",
    "MAX_VAL_SAMPLES = None\n",
    "MAX_TEST_SAMPLES = None\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(SEED)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.backends.cudnn.benchmark = True\n",
    "device\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "b0957487",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-03T18:45:18.952161Z",
     "start_time": "2026-02-03T18:45:18.946144Z"
    }
   },
   "source": [
    "assert TRAIN_DIR.exists(), f'Missing train directory: {TRAIN_DIR}'\n",
    "assert VAL_DIR.exists(), f'Missing val directory: {VAL_DIR}'\n",
    "assert TEST_DIR.exists(), f'Missing test directory: {TEST_DIR}'\n",
    "\n",
    "class_names = sorted([p.name for p in TRAIN_DIR.iterdir() if p.is_dir()])\n",
    "num_classes = len(class_names)\n",
    "class_names[:10], num_classes\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['dry_asphalt_severe',\n",
       "  'dry_asphalt_slight',\n",
       "  'dry_asphalt_smooth',\n",
       "  'dry_concrete_severe',\n",
       "  'dry_concrete_slight',\n",
       "  'dry_concrete_smooth',\n",
       "  'dry_gravel',\n",
       "  'dry_mud',\n",
       "  'fresh_snow',\n",
       "  'ice'],\n",
       " 27)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "e325e1b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-03T18:45:29.025911Z",
     "start_time": "2026-02-03T18:45:19.901035Z"
    }
   },
   "source": [
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "train_tfms = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(IMAGE_SIZE, scale=(0.8, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.05),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std),\n",
    "])\n",
    "\n",
    "eval_tfms = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std),\n",
    "])\n",
    "\n",
    "train_ds = datasets.ImageFolder(TRAIN_DIR, transform=train_tfms)\n",
    "class_to_idx = train_ds.class_to_idx\n",
    "idx_to_class = {v: k for k, v in class_to_idx.items()}\n",
    "\n",
    "def parse_label_from_filename(path: Path) -> str:\n",
    "    stem = path.stem\n",
    "    if '-' not in stem:\n",
    "        return stem\n",
    "    ts, label = stem.split('-', 1)\n",
    "    return label.replace('-', '_')\n",
    "\n",
    "class RSCDFlatDataset(Dataset):\n",
    "    def __init__(self, root_dir: Path, transform=None, class_to_idx=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.samples = sorted([p for p in root_dir.iterdir() if p.is_file()])\n",
    "        self.class_to_idx = class_to_idx or {}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.samples[idx]\n",
    "        label_name = parse_label_from_filename(path)\n",
    "        if label_name not in self.class_to_idx:\n",
    "            raise KeyError(f'Label {label_name} not in class_to_idx')\n",
    "        label = self.class_to_idx[label_name]\n",
    "        img = datasets.folder.default_loader(path)\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, label\n",
    "\n",
    "val_ds = RSCDFlatDataset(VAL_DIR, transform=eval_tfms, class_to_idx=class_to_idx)\n",
    "test_ds = RSCDFlatDataset(TEST_DIR, transform=eval_tfms, class_to_idx=class_to_idx)\n",
    "\n",
    "len(train_ds), len(val_ds), len(test_ds)\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(958941, 19860, 49500)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "5c6b5ec9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-03T18:45:37.626851Z",
     "start_time": "2026-02-03T18:45:37.206586Z"
    }
   },
   "source": [
    "def maybe_subset(ds, max_samples, seed=42):\n",
    "    if max_samples is None or max_samples >= len(ds):\n",
    "        return ds\n",
    "    rng = np.random.default_rng(seed)\n",
    "    indices = rng.choice(len(ds), size=max_samples, replace=False)\n",
    "    return Subset(ds, indices)\n",
    "\n",
    "train_ds_sub = maybe_subset(train_ds, MAX_TRAIN_SAMPLES, SEED)\n",
    "val_ds_sub = maybe_subset(val_ds, MAX_VAL_SAMPLES, SEED)\n",
    "test_ds_sub = maybe_subset(test_ds, MAX_TEST_SAMPLES, SEED)\n",
    "\n",
    "loader_kwargs = dict(batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "train_loader = DataLoader(train_ds_sub, shuffle=True, **loader_kwargs)\n",
    "val_loader = DataLoader(val_ds_sub, shuffle=False, **loader_kwargs)\n",
    "test_loader = DataLoader(test_ds_sub, shuffle=False, **loader_kwargs)\n",
    "\n",
    "next(iter(train_loader))[0].shape\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 3, 224, 224])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "e4f5606c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-03T18:45:40.394781Z",
     "start_time": "2026-02-03T18:45:38.714743Z"
    }
   },
   "source": [
    "import time\n",
    "\n",
    "def time_dataloader(loader, max_batches=5):\n",
    "    start = time.time()\n",
    "    for i, _ in enumerate(loader):\n",
    "        if i + 1 >= max_batches:\n",
    "            break\n",
    "    elapsed = time.time() - start\n",
    "    return elapsed / max_batches\n",
    "\n",
    "avg_batch_time = time_dataloader(train_loader, max_batches=5)\n",
    "print(f'Avg train batch load time: {avg_batch_time:.3f}s')\n",
    "if avg_batch_time > 1.0:\n",
    "    print('Data loading is slow. Try NUM_WORKERS=0 (Windows), increase BATCH_SIZE, or move data to SSD.')\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train batch load time: 0.335s\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "id": "0c396168",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-03T18:45:42.121992Z",
     "start_time": "2026-02-03T18:45:41.951743Z"
    }
   },
   "source": [
    "def build_model(num_classes, use_pretrained=False):\n",
    "    if use_pretrained:\n",
    "        weights = models.ResNet18_Weights.DEFAULT\n",
    "        model = models.resnet18(weights=weights)\n",
    "    else:\n",
    "        model = models.resnet18(weights=None)\n",
    "    in_features = model.fc.in_features\n",
    "    model.fc = nn.Linear(in_features, num_classes)\n",
    "    return model\n",
    "\n",
    "model = build_model(num_classes, USE_PRETRAINED).to(device)\n",
    "model\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=27, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "id": "781040a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-03T18:45:43.814255Z",
     "start_time": "2026-02-03T18:45:43.772071Z"
    }
   },
   "source": [
    "# Fast class weights using targets (no image loading)\n",
    "def compute_class_weights_from_targets(dataset, num_classes):\n",
    "    if isinstance(dataset, Subset):\n",
    "        targets = np.array(dataset.dataset.targets)[dataset.indices]\n",
    "    else:\n",
    "        targets = np.array(dataset.targets)\n",
    "    counts = np.bincount(targets, minlength=num_classes)\n",
    "    counts = np.maximum(counts, 1)\n",
    "    weights = counts.sum() / counts\n",
    "    weights = weights / weights.mean()\n",
    "    return torch.tensor(weights, dtype=torch.float32)\n",
    "\n",
    "class_weights = compute_class_weights_from_targets(train_ds_sub, num_classes).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "scaler = torch.amp.GradScaler(enabled=torch.cuda.is_available(), device=\"cuda\")\n"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "id": "46adaa15",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-03T18:45:44.814126Z",
     "start_time": "2026-02-03T18:45:44.804478Z"
    }
   },
   "source": [
    "def train_one_epoch(model, loader, criterion, optimizer, scaler, log_every=100):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    start_time = time.time()\n",
    "    for batch_idx, (images, labels) in enumerate(loader, start=1):\n",
    "        images = images.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        if batch_idx % log_every == 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            avg_loss = running_loss / total\n",
    "            avg_acc = correct / total\n",
    "            print(f'  [batch {batch_idx}/{len(loader)}] loss {avg_loss:.4f} acc {avg_acc:.4f} | {elapsed:.1f}s elapsed')\n",
    "    return running_loss / total, correct / total\n",
    "\n",
    "def evaluate(model, loader, criterion=None):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    for images, labels in loader:\n",
    "        images = images.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True)\n",
    "        outputs = model(images)\n",
    "        if criterion is not None:\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        all_preds.append(preds.cpu())\n",
    "        all_labels.append(labels.cpu())\n",
    "    all_preds = torch.cat(all_preds)\n",
    "    all_labels = torch.cat(all_labels)\n",
    "    acc = correct / total if total else 0\n",
    "    loss_avg = (running_loss / total) if criterion is not None and total else None\n",
    "    return loss_avg, acc, all_preds, all_labels\n"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "id": "faeebdd4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T02:05:01.676725Z",
     "start_time": "2026-02-03T18:45:47.304942Z"
    }
   },
   "source": [
    "best_val_acc = 0.0\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, scaler)\n",
    "    val_loss, val_acc, _, _ = evaluate(model, val_loader, criterion)\n",
    "    print(f'Epoch {epoch}/{EPOCHS} | train loss {train_loss:.4f} acc {train_acc:.4f} | val loss {val_loss:.4f} acc {val_acc:.4f}')\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save({'model_state': model.state_dict(), 'class_to_idx': class_to_idx}, 'rscd_resnet18.pt')\n",
    "\n",
    "best_val_acc\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\willi\\AppData\\Local\\Temp\\ipykernel_42336\\3496869272.py:11: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch 100/14984] loss 3.0008 acc 0.2022 | 41.6s elapsed\n",
      "  [batch 200/14984] loss 2.9020 acc 0.2298 | 81.8s elapsed\n",
      "  [batch 300/14984] loss 2.8338 acc 0.2480 | 121.6s elapsed\n",
      "  [batch 400/14984] loss 2.7773 acc 0.2601 | 168.3s elapsed\n",
      "  [batch 500/14984] loss 2.7347 acc 0.2732 | 210.7s elapsed\n",
      "  [batch 600/14984] loss 2.6973 acc 0.2839 | 251.6s elapsed\n",
      "  [batch 700/14984] loss 2.6681 acc 0.2920 | 291.8s elapsed\n",
      "  [batch 800/14984] loss 2.6389 acc 0.2982 | 332.8s elapsed\n",
      "  [batch 900/14984] loss 2.6131 acc 0.3042 | 374.7s elapsed\n",
      "  [batch 1000/14984] loss 2.5913 acc 0.3100 | 414.8s elapsed\n",
      "  [batch 1100/14984] loss 2.5672 acc 0.3142 | 454.8s elapsed\n",
      "  [batch 1200/14984] loss 2.5426 acc 0.3201 | 498.9s elapsed\n",
      "  [batch 1300/14984] loss 2.5193 acc 0.3255 | 542.7s elapsed\n",
      "  [batch 1400/14984] loss 2.4969 acc 0.3304 | 587.4s elapsed\n",
      "  [batch 1500/14984] loss 2.4795 acc 0.3340 | 631.4s elapsed\n",
      "  [batch 1600/14984] loss 2.4600 acc 0.3380 | 675.6s elapsed\n",
      "  [batch 1700/14984] loss 2.4438 acc 0.3418 | 719.8s elapsed\n",
      "  [batch 1800/14984] loss 2.4310 acc 0.3450 | 763.9s elapsed\n",
      "  [batch 1900/14984] loss 2.4098 acc 0.3494 | 807.9s elapsed\n",
      "  [batch 2000/14984] loss 2.3939 acc 0.3527 | 852.7s elapsed\n",
      "  [batch 2100/14984] loss 2.3787 acc 0.3563 | 896.8s elapsed\n",
      "  [batch 2200/14984] loss 2.3632 acc 0.3601 | 941.0s elapsed\n",
      "  [batch 2300/14984] loss 2.3507 acc 0.3632 | 986.3s elapsed\n",
      "  [batch 2400/14984] loss 2.3372 acc 0.3665 | 1030.0s elapsed\n",
      "  [batch 2500/14984] loss 2.3215 acc 0.3703 | 1074.1s elapsed\n",
      "  [batch 2600/14984] loss 2.3083 acc 0.3736 | 1117.8s elapsed\n",
      "  [batch 2700/14984] loss 2.2961 acc 0.3762 | 1161.3s elapsed\n",
      "  [batch 2800/14984] loss 2.2848 acc 0.3792 | 1205.8s elapsed\n",
      "  [batch 2900/14984] loss 2.2723 acc 0.3827 | 1251.1s elapsed\n",
      "  [batch 3000/14984] loss 2.2599 acc 0.3857 | 1296.4s elapsed\n",
      "  [batch 3100/14984] loss 2.2460 acc 0.3894 | 1341.8s elapsed\n",
      "  [batch 3200/14984] loss 2.2338 acc 0.3923 | 1386.7s elapsed\n",
      "  [batch 3300/14984] loss 2.2221 acc 0.3952 | 1430.9s elapsed\n",
      "  [batch 3400/14984] loss 2.2103 acc 0.3977 | 1474.9s elapsed\n",
      "  [batch 3500/14984] loss 2.1986 acc 0.4005 | 1519.8s elapsed\n",
      "  [batch 3600/14984] loss 2.1878 acc 0.4029 | 1564.3s elapsed\n",
      "  [batch 3700/14984] loss 2.1787 acc 0.4052 | 1608.5s elapsed\n",
      "  [batch 3800/14984] loss 2.1666 acc 0.4080 | 1652.7s elapsed\n",
      "  [batch 3900/14984] loss 2.1572 acc 0.4102 | 1696.8s elapsed\n",
      "  [batch 4000/14984] loss 2.1466 acc 0.4127 | 1741.4s elapsed\n",
      "  [batch 4100/14984] loss 2.1361 acc 0.4153 | 1785.7s elapsed\n",
      "  [batch 4200/14984] loss 2.1256 acc 0.4182 | 1830.3s elapsed\n",
      "  [batch 4300/14984] loss 2.1164 acc 0.4204 | 1874.2s elapsed\n",
      "  [batch 4400/14984] loss 2.1069 acc 0.4230 | 1918.0s elapsed\n",
      "  [batch 4500/14984] loss 2.0989 acc 0.4251 | 1961.4s elapsed\n",
      "  [batch 4600/14984] loss 2.0896 acc 0.4276 | 2004.8s elapsed\n",
      "  [batch 4700/14984] loss 2.0808 acc 0.4302 | 2049.1s elapsed\n",
      "  [batch 4800/14984] loss 2.0718 acc 0.4324 | 2093.1s elapsed\n",
      "  [batch 4900/14984] loss 2.0645 acc 0.4345 | 2136.6s elapsed\n",
      "  [batch 5000/14984] loss 2.0564 acc 0.4366 | 2180.0s elapsed\n",
      "  [batch 5100/14984] loss 2.0490 acc 0.4387 | 2223.7s elapsed\n",
      "  [batch 5200/14984] loss 2.0414 acc 0.4409 | 2268.2s elapsed\n",
      "  [batch 5300/14984] loss 2.0318 acc 0.4433 | 2314.8s elapsed\n",
      "  [batch 5400/14984] loss 2.0237 acc 0.4454 | 2361.5s elapsed\n",
      "  [batch 5500/14984] loss 2.0167 acc 0.4472 | 2408.2s elapsed\n",
      "  [batch 5600/14984] loss 2.0083 acc 0.4491 | 2454.9s elapsed\n",
      "  [batch 5700/14984] loss 2.0008 acc 0.4510 | 2502.1s elapsed\n",
      "  [batch 5800/14984] loss 1.9930 acc 0.4529 | 2548.8s elapsed\n",
      "  [batch 5900/14984] loss 1.9861 acc 0.4548 | 2595.6s elapsed\n",
      "  [batch 6000/14984] loss 1.9787 acc 0.4566 | 2642.3s elapsed\n",
      "  [batch 6100/14984] loss 1.9722 acc 0.4583 | 2689.0s elapsed\n",
      "  [batch 6200/14984] loss 1.9657 acc 0.4600 | 2736.1s elapsed\n",
      "  [batch 6300/14984] loss 1.9585 acc 0.4616 | 2782.8s elapsed\n",
      "  [batch 6400/14984] loss 1.9515 acc 0.4635 | 2829.6s elapsed\n",
      "  [batch 6500/14984] loss 1.9447 acc 0.4653 | 2876.3s elapsed\n",
      "  [batch 6600/14984] loss 1.9385 acc 0.4669 | 2923.1s elapsed\n",
      "  [batch 6700/14984] loss 1.9318 acc 0.4685 | 2970.4s elapsed\n",
      "  [batch 6800/14984] loss 1.9252 acc 0.4703 | 3017.1s elapsed\n",
      "  [batch 6900/14984] loss 1.9186 acc 0.4720 | 3063.6s elapsed\n",
      "  [batch 7000/14984] loss 1.9127 acc 0.4735 | 3106.5s elapsed\n",
      "  [batch 7100/14984] loss 1.9074 acc 0.4751 | 3145.3s elapsed\n",
      "  [batch 7200/14984] loss 1.9015 acc 0.4765 | 3184.4s elapsed\n",
      "  [batch 7300/14984] loss 1.8964 acc 0.4780 | 3224.3s elapsed\n",
      "  [batch 7400/14984] loss 1.8904 acc 0.4795 | 3263.5s elapsed\n",
      "  [batch 7500/14984] loss 1.8848 acc 0.4810 | 3302.5s elapsed\n",
      "  [batch 7600/14984] loss 1.8786 acc 0.4825 | 3341.7s elapsed\n",
      "  [batch 7700/14984] loss 1.8732 acc 0.4839 | 3380.8s elapsed\n",
      "  [batch 7800/14984] loss 1.8685 acc 0.4853 | 3420.1s elapsed\n",
      "  [batch 7900/14984] loss 1.8634 acc 0.4867 | 3460.0s elapsed\n",
      "  [batch 8000/14984] loss 1.8582 acc 0.4881 | 3499.8s elapsed\n",
      "  [batch 8100/14984] loss 1.8534 acc 0.4894 | 3539.0s elapsed\n",
      "  [batch 8200/14984] loss 1.8484 acc 0.4909 | 3578.5s elapsed\n",
      "  [batch 8300/14984] loss 1.8437 acc 0.4920 | 3622.8s elapsed\n",
      "  [batch 8400/14984] loss 1.8394 acc 0.4933 | 3667.3s elapsed\n",
      "  [batch 8500/14984] loss 1.8342 acc 0.4945 | 3711.4s elapsed\n",
      "  [batch 8600/14984] loss 1.8297 acc 0.4958 | 3755.2s elapsed\n",
      "  [batch 8700/14984] loss 1.8251 acc 0.4969 | 3799.8s elapsed\n",
      "  [batch 8800/14984] loss 1.8204 acc 0.4982 | 3843.8s elapsed\n",
      "  [batch 8900/14984] loss 1.8161 acc 0.4994 | 3888.9s elapsed\n",
      "  [batch 9000/14984] loss 1.8113 acc 0.5006 | 3932.8s elapsed\n",
      "  [batch 9100/14984] loss 1.8061 acc 0.5020 | 3976.9s elapsed\n",
      "  [batch 9200/14984] loss 1.8013 acc 0.5034 | 4021.1s elapsed\n",
      "  [batch 9300/14984] loss 1.7965 acc 0.5046 | 4065.0s elapsed\n",
      "  [batch 9400/14984] loss 1.7929 acc 0.5058 | 4108.8s elapsed\n",
      "  [batch 9500/14984] loss 1.7887 acc 0.5071 | 4152.2s elapsed\n",
      "  [batch 9600/14984] loss 1.7847 acc 0.5083 | 4195.7s elapsed\n",
      "  [batch 9700/14984] loss 1.7806 acc 0.5095 | 4239.4s elapsed\n",
      "  [batch 9800/14984] loss 1.7764 acc 0.5106 | 4283.8s elapsed\n",
      "  [batch 9900/14984] loss 1.7719 acc 0.5117 | 4327.7s elapsed\n",
      "  [batch 10000/14984] loss 1.7674 acc 0.5129 | 4372.5s elapsed\n",
      "  [batch 10100/14984] loss 1.7631 acc 0.5141 | 4416.1s elapsed\n",
      "  [batch 10200/14984] loss 1.7591 acc 0.5152 | 4459.9s elapsed\n",
      "  [batch 10300/14984] loss 1.7551 acc 0.5163 | 4503.6s elapsed\n",
      "  [batch 10400/14984] loss 1.7512 acc 0.5173 | 4547.8s elapsed\n",
      "  [batch 10500/14984] loss 1.7473 acc 0.5183 | 4592.6s elapsed\n",
      "  [batch 10600/14984] loss 1.7431 acc 0.5195 | 4636.4s elapsed\n",
      "  [batch 10700/14984] loss 1.7392 acc 0.5207 | 4679.8s elapsed\n",
      "  [batch 10800/14984] loss 1.7356 acc 0.5217 | 4723.8s elapsed\n",
      "  [batch 10900/14984] loss 1.7320 acc 0.5228 | 4767.8s elapsed\n",
      "  [batch 11000/14984] loss 1.7279 acc 0.5239 | 4812.0s elapsed\n",
      "  [batch 11100/14984] loss 1.7235 acc 0.5250 | 4857.0s elapsed\n",
      "  [batch 11200/14984] loss 1.7197 acc 0.5260 | 4901.1s elapsed\n",
      "  [batch 11300/14984] loss 1.7159 acc 0.5270 | 4944.9s elapsed\n",
      "  [batch 11400/14984] loss 1.7122 acc 0.5280 | 4988.6s elapsed\n",
      "  [batch 11500/14984] loss 1.7090 acc 0.5289 | 5032.6s elapsed\n",
      "  [batch 11600/14984] loss 1.7055 acc 0.5298 | 5077.4s elapsed\n",
      "  [batch 11700/14984] loss 1.7022 acc 0.5308 | 5121.5s elapsed\n",
      "  [batch 11800/14984] loss 1.6988 acc 0.5317 | 5165.1s elapsed\n",
      "  [batch 11900/14984] loss 1.6955 acc 0.5326 | 5208.7s elapsed\n",
      "  [batch 12000/14984] loss 1.6921 acc 0.5336 | 5253.0s elapsed\n",
      "  [batch 12100/14984] loss 1.6888 acc 0.5345 | 5297.6s elapsed\n",
      "  [batch 12200/14984] loss 1.6859 acc 0.5353 | 5341.7s elapsed\n",
      "  [batch 12300/14984] loss 1.6824 acc 0.5361 | 5385.5s elapsed\n",
      "  [batch 12400/14984] loss 1.6791 acc 0.5370 | 5429.7s elapsed\n",
      "  [batch 12500/14984] loss 1.6758 acc 0.5379 | 5473.7s elapsed\n",
      "  [batch 12600/14984] loss 1.6727 acc 0.5388 | 5514.3s elapsed\n",
      "  [batch 12700/14984] loss 1.6692 acc 0.5398 | 5553.7s elapsed\n",
      "  [batch 12800/14984] loss 1.6659 acc 0.5408 | 5598.0s elapsed\n",
      "  [batch 12900/14984] loss 1.6626 acc 0.5416 | 5642.9s elapsed\n",
      "  [batch 13000/14984] loss 1.6593 acc 0.5424 | 5686.2s elapsed\n",
      "  [batch 13100/14984] loss 1.6564 acc 0.5432 | 5730.6s elapsed\n",
      "  [batch 13200/14984] loss 1.6534 acc 0.5439 | 5774.4s elapsed\n",
      "  [batch 13300/14984] loss 1.6500 acc 0.5448 | 5818.1s elapsed\n",
      "  [batch 13400/14984] loss 1.6471 acc 0.5456 | 5861.9s elapsed\n",
      "  [batch 13500/14984] loss 1.6445 acc 0.5464 | 5905.8s elapsed\n",
      "  [batch 13600/14984] loss 1.6412 acc 0.5473 | 5952.9s elapsed\n",
      "  [batch 13700/14984] loss 1.6382 acc 0.5481 | 5996.7s elapsed\n",
      "  [batch 13800/14984] loss 1.6354 acc 0.5488 | 6040.5s elapsed\n",
      "  [batch 13900/14984] loss 1.6321 acc 0.5496 | 6084.4s elapsed\n",
      "  [batch 14000/14984] loss 1.6293 acc 0.5503 | 6128.0s elapsed\n",
      "  [batch 14100/14984] loss 1.6263 acc 0.5511 | 6172.0s elapsed\n",
      "  [batch 14200/14984] loss 1.6237 acc 0.5518 | 6216.4s elapsed\n",
      "  [batch 14300/14984] loss 1.6207 acc 0.5526 | 6260.2s elapsed\n",
      "  [batch 14400/14984] loss 1.6176 acc 0.5534 | 6303.9s elapsed\n",
      "  [batch 14500/14984] loss 1.6150 acc 0.5541 | 6347.8s elapsed\n",
      "  [batch 14600/14984] loss 1.6126 acc 0.5548 | 6391.6s elapsed\n",
      "  [batch 14700/14984] loss 1.6096 acc 0.5556 | 6435.8s elapsed\n",
      "  [batch 14800/14984] loss 1.6067 acc 0.5563 | 6479.7s elapsed\n",
      "  [batch 14900/14984] loss 1.6043 acc 0.5571 | 6525.0s elapsed\n",
      "Epoch 1/3 | train loss 1.6021 acc 0.5577 | val loss 1.1905 acc 0.6402\n",
      "  [batch 100/14984] loss 1.2026 acc 0.6666 | 45.2s elapsed\n",
      "  [batch 200/14984] loss 1.2077 acc 0.6680 | 89.3s elapsed\n",
      "  [batch 300/14984] loss 1.1826 acc 0.6699 | 132.9s elapsed\n",
      "  [batch 400/14984] loss 1.1767 acc 0.6705 | 178.4s elapsed\n",
      "  [batch 500/14984] loss 1.1821 acc 0.6694 | 222.1s elapsed\n",
      "  [batch 600/14984] loss 1.1777 acc 0.6703 | 265.4s elapsed\n",
      "  [batch 700/14984] loss 1.1749 acc 0.6700 | 312.1s elapsed\n",
      "  [batch 800/14984] loss 1.1776 acc 0.6718 | 359.1s elapsed\n",
      "  [batch 900/14984] loss 1.1775 acc 0.6722 | 405.4s elapsed\n",
      "  [batch 1000/14984] loss 1.1702 acc 0.6742 | 451.9s elapsed\n",
      "  [batch 1100/14984] loss 1.1713 acc 0.6747 | 499.9s elapsed\n",
      "  [batch 1200/14984] loss 1.1656 acc 0.6743 | 546.9s elapsed\n",
      "  [batch 1300/14984] loss 1.1641 acc 0.6750 | 591.1s elapsed\n",
      "  [batch 1400/14984] loss 1.1643 acc 0.6749 | 634.4s elapsed\n",
      "  [batch 1500/14984] loss 1.1620 acc 0.6754 | 677.5s elapsed\n",
      "  [batch 1600/14984] loss 1.1622 acc 0.6752 | 721.4s elapsed\n",
      "  [batch 1700/14984] loss 1.1571 acc 0.6757 | 764.4s elapsed\n",
      "  [batch 1800/14984] loss 1.1600 acc 0.6756 | 807.6s elapsed\n",
      "  [batch 1900/14984] loss 1.1609 acc 0.6756 | 850.7s elapsed\n",
      "  [batch 2000/14984] loss 1.1581 acc 0.6763 | 893.5s elapsed\n",
      "  [batch 2100/14984] loss 1.1567 acc 0.6769 | 936.4s elapsed\n",
      "  [batch 2200/14984] loss 1.1547 acc 0.6775 | 979.3s elapsed\n",
      "  [batch 2300/14984] loss 1.1537 acc 0.6777 | 1022.3s elapsed\n",
      "  [batch 2400/14984] loss 1.1524 acc 0.6778 | 1065.7s elapsed\n",
      "  [batch 2500/14984] loss 1.1504 acc 0.6783 | 1108.5s elapsed\n",
      "  [batch 2600/14984] loss 1.1473 acc 0.6794 | 1151.6s elapsed\n",
      "  [batch 2700/14984] loss 1.1452 acc 0.6799 | 1194.5s elapsed\n",
      "  [batch 2800/14984] loss 1.1442 acc 0.6799 | 1237.2s elapsed\n",
      "  [batch 2900/14984] loss 1.1468 acc 0.6799 | 1280.2s elapsed\n",
      "  [batch 3000/14984] loss 1.1426 acc 0.6806 | 1324.0s elapsed\n",
      "  [batch 3100/14984] loss 1.1416 acc 0.6809 | 1366.8s elapsed\n",
      "  [batch 3200/14984] loss 1.1390 acc 0.6815 | 1409.6s elapsed\n",
      "  [batch 3300/14984] loss 1.1384 acc 0.6816 | 1452.9s elapsed\n",
      "  [batch 3400/14984] loss 1.1387 acc 0.6815 | 1495.9s elapsed\n",
      "  [batch 3500/14984] loss 1.1382 acc 0.6816 | 1538.7s elapsed\n",
      "  [batch 3600/14984] loss 1.1375 acc 0.6816 | 1582.1s elapsed\n",
      "  [batch 3700/14984] loss 1.1365 acc 0.6819 | 1624.9s elapsed\n",
      "  [batch 3800/14984] loss 1.1351 acc 0.6821 | 1668.1s elapsed\n",
      "  [batch 3900/14984] loss 1.1342 acc 0.6823 | 1711.4s elapsed\n",
      "  [batch 4000/14984] loss 1.1336 acc 0.6826 | 1754.5s elapsed\n",
      "  [batch 4100/14984] loss 1.1333 acc 0.6828 | 1797.6s elapsed\n",
      "  [batch 4200/14984] loss 1.1322 acc 0.6831 | 1841.3s elapsed\n",
      "  [batch 4300/14984] loss 1.1328 acc 0.6830 | 1884.3s elapsed\n",
      "  [batch 4400/14984] loss 1.1314 acc 0.6832 | 1927.3s elapsed\n",
      "  [batch 4500/14984] loss 1.1305 acc 0.6837 | 1970.4s elapsed\n",
      "  [batch 4600/14984] loss 1.1288 acc 0.6839 | 2013.3s elapsed\n",
      "  [batch 4700/14984] loss 1.1281 acc 0.6843 | 2056.5s elapsed\n",
      "  [batch 4800/14984] loss 1.1265 acc 0.6846 | 2099.6s elapsed\n",
      "  [batch 4900/14984] loss 1.1263 acc 0.6846 | 2143.1s elapsed\n",
      "  [batch 5000/14984] loss 1.1250 acc 0.6849 | 2186.0s elapsed\n",
      "  [batch 5100/14984] loss 1.1245 acc 0.6853 | 2229.0s elapsed\n",
      "  [batch 5200/14984] loss 1.1227 acc 0.6856 | 2272.0s elapsed\n",
      "  [batch 5300/14984] loss 1.1221 acc 0.6858 | 2315.1s elapsed\n",
      "  [batch 5400/14984] loss 1.1213 acc 0.6860 | 2358.8s elapsed\n",
      "  [batch 5500/14984] loss 1.1205 acc 0.6863 | 2402.1s elapsed\n",
      "  [batch 5600/14984] loss 1.1204 acc 0.6863 | 2445.2s elapsed\n",
      "  [batch 5700/14984] loss 1.1205 acc 0.6863 | 2488.6s elapsed\n",
      "  [batch 5800/14984] loss 1.1205 acc 0.6865 | 2531.8s elapsed\n",
      "  [batch 5900/14984] loss 1.1193 acc 0.6869 | 2575.0s elapsed\n",
      "  [batch 6000/14984] loss 1.1183 acc 0.6871 | 2618.7s elapsed\n",
      "  [batch 6100/14984] loss 1.1174 acc 0.6874 | 2662.2s elapsed\n",
      "  [batch 6200/14984] loss 1.1161 acc 0.6877 | 2705.5s elapsed\n",
      "  [batch 6300/14984] loss 1.1146 acc 0.6880 | 2749.4s elapsed\n",
      "  [batch 6400/14984] loss 1.1135 acc 0.6885 | 2793.3s elapsed\n",
      "  [batch 6500/14984] loss 1.1128 acc 0.6889 | 2838.1s elapsed\n",
      "  [batch 6600/14984] loss 1.1112 acc 0.6892 | 2881.6s elapsed\n",
      "  [batch 6700/14984] loss 1.1105 acc 0.6895 | 2924.9s elapsed\n",
      "  [batch 6800/14984] loss 1.1084 acc 0.6898 | 2968.1s elapsed\n",
      "  [batch 6900/14984] loss 1.1070 acc 0.6901 | 3011.3s elapsed\n",
      "  [batch 7000/14984] loss 1.1065 acc 0.6901 | 3054.6s elapsed\n",
      "  [batch 7100/14984] loss 1.1054 acc 0.6904 | 3098.2s elapsed\n",
      "  [batch 7200/14984] loss 1.1043 acc 0.6907 | 3143.2s elapsed\n",
      "  [batch 7300/14984] loss 1.1032 acc 0.6910 | 3186.5s elapsed\n",
      "  [batch 7400/14984] loss 1.1021 acc 0.6912 | 3229.9s elapsed\n",
      "  [batch 7500/14984] loss 1.1011 acc 0.6915 | 3273.2s elapsed\n",
      "  [batch 7600/14984] loss 1.1000 acc 0.6918 | 3317.1s elapsed\n",
      "  [batch 7700/14984] loss 1.0990 acc 0.6921 | 3360.5s elapsed\n",
      "  [batch 7800/14984] loss 1.0986 acc 0.6924 | 3403.8s elapsed\n",
      "  [batch 7900/14984] loss 1.0971 acc 0.6927 | 3447.1s elapsed\n",
      "  [batch 8000/14984] loss 1.0961 acc 0.6930 | 3490.4s elapsed\n",
      "  [batch 8100/14984] loss 1.0951 acc 0.6932 | 3534.4s elapsed\n",
      "  [batch 8200/14984] loss 1.0943 acc 0.6935 | 3577.8s elapsed\n",
      "  [batch 8300/14984] loss 1.0935 acc 0.6938 | 3621.1s elapsed\n",
      "  [batch 8400/14984] loss 1.0931 acc 0.6941 | 3664.4s elapsed\n",
      "  [batch 8500/14984] loss 1.0918 acc 0.6944 | 3707.3s elapsed\n",
      "  [batch 8600/14984] loss 1.0910 acc 0.6946 | 3750.9s elapsed\n",
      "  [batch 8700/14984] loss 1.0899 acc 0.6949 | 3794.1s elapsed\n",
      "  [batch 8800/14984] loss 1.0888 acc 0.6953 | 3837.2s elapsed\n",
      "  [batch 8900/14984] loss 1.0881 acc 0.6956 | 3880.1s elapsed\n",
      "  [batch 9000/14984] loss 1.0872 acc 0.6958 | 3923.0s elapsed\n",
      "  [batch 9100/14984] loss 1.0865 acc 0.6961 | 3966.2s elapsed\n",
      "  [batch 9200/14984] loss 1.0855 acc 0.6963 | 4009.8s elapsed\n",
      "  [batch 9300/14984] loss 1.0847 acc 0.6966 | 4052.9s elapsed\n",
      "  [batch 9400/14984] loss 1.0836 acc 0.6969 | 4095.9s elapsed\n",
      "  [batch 9500/14984] loss 1.0832 acc 0.6971 | 4139.2s elapsed\n",
      "  [batch 9600/14984] loss 1.0820 acc 0.6974 | 4182.3s elapsed\n",
      "  [batch 9700/14984] loss 1.0809 acc 0.6977 | 4226.1s elapsed\n",
      "  [batch 9800/14984] loss 1.0800 acc 0.6981 | 4269.2s elapsed\n",
      "  [batch 9900/14984] loss 1.0792 acc 0.6984 | 4312.6s elapsed\n",
      "  [batch 10000/14984] loss 1.0782 acc 0.6987 | 4355.8s elapsed\n",
      "  [batch 10100/14984] loss 1.0780 acc 0.6988 | 4399.0s elapsed\n",
      "  [batch 10200/14984] loss 1.0779 acc 0.6990 | 4443.1s elapsed\n",
      "  [batch 10300/14984] loss 1.0770 acc 0.6992 | 4486.5s elapsed\n",
      "  [batch 10400/14984] loss 1.0762 acc 0.6994 | 4529.7s elapsed\n",
      "  [batch 10500/14984] loss 1.0752 acc 0.6996 | 4572.9s elapsed\n",
      "  [batch 10600/14984] loss 1.0744 acc 0.6999 | 4616.2s elapsed\n",
      "  [batch 10700/14984] loss 1.0734 acc 0.7002 | 4659.9s elapsed\n",
      "  [batch 10800/14984] loss 1.0725 acc 0.7004 | 4703.1s elapsed\n",
      "  [batch 10900/14984] loss 1.0715 acc 0.7006 | 4746.6s elapsed\n",
      "  [batch 11000/14984] loss 1.0704 acc 0.7009 | 4789.7s elapsed\n",
      "  [batch 11100/14984] loss 1.0694 acc 0.7012 | 4833.2s elapsed\n",
      "  [batch 11200/14984] loss 1.0685 acc 0.7014 | 4877.1s elapsed\n",
      "  [batch 11300/14984] loss 1.0679 acc 0.7017 | 4920.4s elapsed\n",
      "  [batch 11400/14984] loss 1.0668 acc 0.7020 | 4963.8s elapsed\n",
      "  [batch 11500/14984] loss 1.0657 acc 0.7022 | 5007.2s elapsed\n",
      "  [batch 11600/14984] loss 1.0648 acc 0.7025 | 5050.4s elapsed\n",
      "  [batch 11700/14984] loss 1.0643 acc 0.7027 | 5093.9s elapsed\n",
      "  [batch 11800/14984] loss 1.0633 acc 0.7030 | 5136.8s elapsed\n",
      "  [batch 11900/14984] loss 1.0625 acc 0.7032 | 5179.6s elapsed\n",
      "  [batch 12000/14984] loss 1.0619 acc 0.7034 | 5222.5s elapsed\n",
      "  [batch 12100/14984] loss 1.0610 acc 0.7037 | 5265.6s elapsed\n",
      "  [batch 12200/14984] loss 1.0601 acc 0.7039 | 5309.1s elapsed\n",
      "  [batch 12300/14984] loss 1.0599 acc 0.7041 | 5352.1s elapsed\n",
      "  [batch 12400/14984] loss 1.0589 acc 0.7043 | 5395.2s elapsed\n",
      "  [batch 12500/14984] loss 1.0579 acc 0.7046 | 5438.2s elapsed\n",
      "  [batch 12600/14984] loss 1.0570 acc 0.7049 | 5481.1s elapsed\n",
      "  [batch 12700/14984] loss 1.0563 acc 0.7051 | 5524.2s elapsed\n",
      "  [batch 12800/14984] loss 1.0553 acc 0.7053 | 5567.9s elapsed\n",
      "  [batch 12900/14984] loss 1.0546 acc 0.7056 | 5611.1s elapsed\n",
      "  [batch 13000/14984] loss 1.0537 acc 0.7059 | 5654.3s elapsed\n",
      "  [batch 13100/14984] loss 1.0531 acc 0.7060 | 5697.5s elapsed\n",
      "  [batch 13200/14984] loss 1.0526 acc 0.7063 | 5740.5s elapsed\n",
      "  [batch 13300/14984] loss 1.0521 acc 0.7064 | 5784.2s elapsed\n",
      "  [batch 13400/14984] loss 1.0510 acc 0.7067 | 5827.3s elapsed\n",
      "  [batch 13500/14984] loss 1.0503 acc 0.7069 | 5870.5s elapsed\n",
      "  [batch 13600/14984] loss 1.0495 acc 0.7072 | 5913.5s elapsed\n",
      "  [batch 13700/14984] loss 1.0492 acc 0.7074 | 5956.8s elapsed\n",
      "  [batch 13800/14984] loss 1.0485 acc 0.7076 | 6000.4s elapsed\n",
      "  [batch 13900/14984] loss 1.0477 acc 0.7078 | 6043.8s elapsed\n",
      "  [batch 14000/14984] loss 1.0473 acc 0.7079 | 6087.1s elapsed\n",
      "  [batch 14100/14984] loss 1.0467 acc 0.7082 | 6130.3s elapsed\n",
      "  [batch 14200/14984] loss 1.0457 acc 0.7084 | 6173.3s elapsed\n",
      "  [batch 14300/14984] loss 1.0448 acc 0.7086 | 6220.5s elapsed\n",
      "  [batch 14400/14984] loss 1.0442 acc 0.7088 | 6303.5s elapsed\n",
      "  [batch 14500/14984] loss 1.0437 acc 0.7089 | 6386.5s elapsed\n",
      "  [batch 14600/14984] loss 1.0427 acc 0.7092 | 6468.4s elapsed\n",
      "  [batch 14700/14984] loss 1.0422 acc 0.7094 | 6550.4s elapsed\n",
      "  [batch 14800/14984] loss 1.0413 acc 0.7096 | 6632.3s elapsed\n",
      "  [batch 14900/14984] loss 1.0407 acc 0.7098 | 6718.6s elapsed\n",
      "Epoch 2/3 | train loss 1.0401 acc 0.7099 | val loss 0.9044 acc 0.7144\n",
      "  [batch 100/14984] loss 0.8874 acc 0.7444 | 80.9s elapsed\n",
      "  [batch 200/14984] loss 0.8858 acc 0.7472 | 161.6s elapsed\n",
      "  [batch 300/14984] loss 0.9041 acc 0.7414 | 242.2s elapsed\n",
      "  [batch 400/14984] loss 0.8958 acc 0.7434 | 322.9s elapsed\n",
      "  [batch 500/14984] loss 0.8939 acc 0.7450 | 403.0s elapsed\n",
      "  [batch 600/14984] loss 0.8966 acc 0.7445 | 483.0s elapsed\n",
      "  [batch 700/14984] loss 0.8889 acc 0.7452 | 562.7s elapsed\n",
      "  [batch 800/14984] loss 0.8869 acc 0.7446 | 642.8s elapsed\n",
      "  [batch 900/14984] loss 0.8983 acc 0.7429 | 723.2s elapsed\n",
      "  [batch 1000/14984] loss 0.8920 acc 0.7442 | 803.9s elapsed\n",
      "  [batch 1100/14984] loss 0.8921 acc 0.7443 | 884.3s elapsed\n",
      "  [batch 1200/14984] loss 0.8968 acc 0.7433 | 964.6s elapsed\n",
      "  [batch 1300/14984] loss 0.9009 acc 0.7425 | 1044.3s elapsed\n",
      "  [batch 1400/14984] loss 0.8999 acc 0.7429 | 1124.7s elapsed\n",
      "  [batch 1500/14984] loss 0.8984 acc 0.7434 | 1205.1s elapsed\n",
      "  [batch 1600/14984] loss 0.9020 acc 0.7434 | 1284.9s elapsed\n",
      "  [batch 1700/14984] loss 0.9037 acc 0.7436 | 1364.4s elapsed\n",
      "  [batch 1800/14984] loss 0.9045 acc 0.7435 | 1445.3s elapsed\n",
      "  [batch 1900/14984] loss 0.9012 acc 0.7443 | 1524.7s elapsed\n",
      "  [batch 2000/14984] loss 0.9005 acc 0.7443 | 1605.0s elapsed\n",
      "  [batch 2100/14984] loss 0.8999 acc 0.7451 | 1683.5s elapsed\n",
      "  [batch 2200/14984] loss 0.8994 acc 0.7450 | 1762.5s elapsed\n",
      "  [batch 2300/14984] loss 0.9003 acc 0.7452 | 1843.2s elapsed\n",
      "  [batch 2400/14984] loss 0.8999 acc 0.7455 | 1925.3s elapsed\n",
      "  [batch 2500/14984] loss 0.8994 acc 0.7456 | 2006.5s elapsed\n",
      "  [batch 2600/14984] loss 0.9013 acc 0.7455 | 2087.4s elapsed\n",
      "  [batch 2700/14984] loss 0.9002 acc 0.7459 | 2168.6s elapsed\n",
      "  [batch 2800/14984] loss 0.8994 acc 0.7457 | 2249.8s elapsed\n",
      "  [batch 2900/14984] loss 0.8993 acc 0.7460 | 2331.4s elapsed\n",
      "  [batch 3000/14984] loss 0.8984 acc 0.7461 | 2421.9s elapsed\n",
      "  [batch 3100/14984] loss 0.8987 acc 0.7460 | 2505.6s elapsed\n",
      "  [batch 3200/14984] loss 0.8986 acc 0.7462 | 2589.3s elapsed\n",
      "  [batch 3300/14984] loss 0.8977 acc 0.7466 | 2673.5s elapsed\n",
      "  [batch 3400/14984] loss 0.8965 acc 0.7468 | 2757.3s elapsed\n",
      "  [batch 3500/14984] loss 0.8945 acc 0.7473 | 2840.9s elapsed\n",
      "  [batch 3600/14984] loss 0.8935 acc 0.7476 | 2924.4s elapsed\n",
      "  [batch 3700/14984] loss 0.8939 acc 0.7477 | 3010.6s elapsed\n",
      "  [batch 3800/14984] loss 0.8927 acc 0.7478 | 3094.8s elapsed\n",
      "  [batch 3900/14984] loss 0.8923 acc 0.7480 | 3179.2s elapsed\n",
      "  [batch 4000/14984] loss 0.8912 acc 0.7485 | 3263.2s elapsed\n",
      "  [batch 4100/14984] loss 0.8900 acc 0.7489 | 3347.5s elapsed\n",
      "  [batch 4200/14984] loss 0.8890 acc 0.7490 | 3431.0s elapsed\n",
      "  [batch 4300/14984] loss 0.8889 acc 0.7493 | 3514.3s elapsed\n",
      "  [batch 4400/14984] loss 0.8885 acc 0.7494 | 3597.0s elapsed\n",
      "  [batch 4500/14984] loss 0.8886 acc 0.7494 | 3678.8s elapsed\n",
      "  [batch 4600/14984] loss 0.8882 acc 0.7495 | 3759.8s elapsed\n",
      "  [batch 4700/14984] loss 0.8883 acc 0.7494 | 3840.9s elapsed\n",
      "  [batch 4800/14984] loss 0.8873 acc 0.7495 | 3921.6s elapsed\n",
      "  [batch 4900/14984] loss 0.8865 acc 0.7497 | 4002.6s elapsed\n",
      "  [batch 5000/14984] loss 0.8862 acc 0.7499 | 4083.2s elapsed\n",
      "  [batch 5100/14984] loss 0.8860 acc 0.7500 | 4163.9s elapsed\n",
      "  [batch 5200/14984] loss 0.8855 acc 0.7502 | 4245.3s elapsed\n",
      "  [batch 5300/14984] loss 0.8849 acc 0.7503 | 4325.8s elapsed\n",
      "  [batch 5400/14984] loss 0.8845 acc 0.7505 | 4406.3s elapsed\n",
      "  [batch 5500/14984] loss 0.8833 acc 0.7508 | 4486.6s elapsed\n",
      "  [batch 5600/14984] loss 0.8833 acc 0.7509 | 4567.5s elapsed\n",
      "  [batch 5700/14984] loss 0.8826 acc 0.7512 | 4647.5s elapsed\n",
      "  [batch 5800/14984] loss 0.8826 acc 0.7512 | 4727.5s elapsed\n",
      "  [batch 5900/14984] loss 0.8829 acc 0.7514 | 4808.0s elapsed\n",
      "  [batch 6000/14984] loss 0.8817 acc 0.7517 | 4888.8s elapsed\n",
      "  [batch 6100/14984] loss 0.8812 acc 0.7519 | 4970.9s elapsed\n",
      "  [batch 6200/14984] loss 0.8806 acc 0.7521 | 5051.8s elapsed\n",
      "  [batch 6300/14984] loss 0.8800 acc 0.7522 | 5132.8s elapsed\n",
      "  [batch 6400/14984] loss 0.8800 acc 0.7523 | 5213.9s elapsed\n",
      "  [batch 6500/14984] loss 0.8791 acc 0.7525 | 5295.1s elapsed\n",
      "  [batch 6600/14984] loss 0.8785 acc 0.7526 | 5376.3s elapsed\n",
      "  [batch 6700/14984] loss 0.8783 acc 0.7528 | 5457.0s elapsed\n",
      "  [batch 6800/14984] loss 0.8780 acc 0.7528 | 5538.9s elapsed\n",
      "  [batch 6900/14984] loss 0.8778 acc 0.7528 | 5620.5s elapsed\n",
      "  [batch 7000/14984] loss 0.8776 acc 0.7529 | 5701.9s elapsed\n",
      "  [batch 7100/14984] loss 0.8775 acc 0.7529 | 5783.7s elapsed\n",
      "  [batch 7200/14984] loss 0.8768 acc 0.7531 | 5865.8s elapsed\n",
      "  [batch 7300/14984] loss 0.8768 acc 0.7531 | 5948.0s elapsed\n",
      "  [batch 7400/14984] loss 0.8764 acc 0.7532 | 6039.4s elapsed\n",
      "  [batch 7500/14984] loss 0.8763 acc 0.7534 | 6142.9s elapsed\n",
      "  [batch 7600/14984] loss 0.8761 acc 0.7535 | 6227.9s elapsed\n",
      "  [batch 7700/14984] loss 0.8753 acc 0.7536 | 6311.7s elapsed\n",
      "  [batch 7800/14984] loss 0.8750 acc 0.7538 | 6397.3s elapsed\n",
      "  [batch 7900/14984] loss 0.8747 acc 0.7539 | 6481.6s elapsed\n",
      "  [batch 8000/14984] loss 0.8744 acc 0.7539 | 6566.1s elapsed\n",
      "  [batch 8100/14984] loss 0.8739 acc 0.7540 | 6652.1s elapsed\n",
      "  [batch 8200/14984] loss 0.8731 acc 0.7541 | 6737.6s elapsed\n",
      "  [batch 8300/14984] loss 0.8733 acc 0.7543 | 6824.4s elapsed\n",
      "  [batch 8400/14984] loss 0.8732 acc 0.7543 | 6908.7s elapsed\n",
      "  [batch 8500/14984] loss 0.8719 acc 0.7546 | 6992.5s elapsed\n",
      "  [batch 8600/14984] loss 0.8718 acc 0.7546 | 7078.0s elapsed\n",
      "  [batch 8700/14984] loss 0.8714 acc 0.7547 | 7162.0s elapsed\n",
      "  [batch 8800/14984] loss 0.8717 acc 0.7548 | 7246.3s elapsed\n",
      "  [batch 8900/14984] loss 0.8713 acc 0.7550 | 7331.1s elapsed\n",
      "  [batch 9000/14984] loss 0.8716 acc 0.7550 | 7415.3s elapsed\n",
      "  [batch 9100/14984] loss 0.8710 acc 0.7553 | 7500.2s elapsed\n",
      "  [batch 9200/14984] loss 0.8702 acc 0.7554 | 7584.4s elapsed\n",
      "  [batch 9300/14984] loss 0.8695 acc 0.7556 | 7668.8s elapsed\n",
      "  [batch 9400/14984] loss 0.8694 acc 0.7556 | 7753.5s elapsed\n",
      "  [batch 9500/14984] loss 0.8690 acc 0.7557 | 7838.3s elapsed\n",
      "  [batch 9600/14984] loss 0.8685 acc 0.7559 | 7923.2s elapsed\n",
      "  [batch 9700/14984] loss 0.8680 acc 0.7560 | 8007.2s elapsed\n",
      "  [batch 9800/14984] loss 0.8679 acc 0.7561 | 8091.7s elapsed\n",
      "  [batch 9900/14984] loss 0.8673 acc 0.7564 | 8176.9s elapsed\n",
      "  [batch 10000/14984] loss 0.8671 acc 0.7565 | 8261.8s elapsed\n",
      "  [batch 10100/14984] loss 0.8667 acc 0.7567 | 8345.7s elapsed\n",
      "  [batch 10200/14984] loss 0.8656 acc 0.7569 | 8430.1s elapsed\n",
      "  [batch 10300/14984] loss 0.8649 acc 0.7571 | 8514.6s elapsed\n",
      "  [batch 10400/14984] loss 0.8643 acc 0.7573 | 8599.4s elapsed\n",
      "  [batch 10500/14984] loss 0.8643 acc 0.7573 | 8683.7s elapsed\n",
      "  [batch 10600/14984] loss 0.8634 acc 0.7575 | 8768.1s elapsed\n",
      "  [batch 10700/14984] loss 0.8630 acc 0.7577 | 8853.0s elapsed\n",
      "  [batch 10800/14984] loss 0.8628 acc 0.7577 | 8936.7s elapsed\n",
      "  [batch 10900/14984] loss 0.8626 acc 0.7579 | 9021.3s elapsed\n",
      "  [batch 11000/14984] loss 0.8621 acc 0.7581 | 9105.8s elapsed\n",
      "  [batch 11100/14984] loss 0.8614 acc 0.7582 | 9190.2s elapsed\n",
      "  [batch 11200/14984] loss 0.8611 acc 0.7584 | 9275.2s elapsed\n",
      "  [batch 11300/14984] loss 0.8603 acc 0.7585 | 9359.0s elapsed\n",
      "  [batch 11400/14984] loss 0.8596 acc 0.7586 | 9442.6s elapsed\n",
      "  [batch 11500/14984] loss 0.8598 acc 0.7587 | 9526.3s elapsed\n",
      "  [batch 11600/14984] loss 0.8596 acc 0.7588 | 9611.1s elapsed\n",
      "  [batch 11700/14984] loss 0.8591 acc 0.7589 | 9695.5s elapsed\n",
      "  [batch 11800/14984] loss 0.8587 acc 0.7590 | 9779.7s elapsed\n",
      "  [batch 11900/14984] loss 0.8582 acc 0.7591 | 9865.8s elapsed\n",
      "  [batch 12000/14984] loss 0.8580 acc 0.7592 | 9951.6s elapsed\n",
      "  [batch 12100/14984] loss 0.8574 acc 0.7594 | 10036.3s elapsed\n",
      "  [batch 12200/14984] loss 0.8568 acc 0.7595 | 10120.6s elapsed\n",
      "  [batch 12300/14984] loss 0.8563 acc 0.7596 | 10208.2s elapsed\n",
      "  [batch 12400/14984] loss 0.8554 acc 0.7597 | 10293.5s elapsed\n",
      "  [batch 12500/14984] loss 0.8552 acc 0.7598 | 10378.1s elapsed\n",
      "  [batch 12600/14984] loss 0.8549 acc 0.7599 | 10462.5s elapsed\n",
      "  [batch 12700/14984] loss 0.8543 acc 0.7601 | 10546.5s elapsed\n",
      "  [batch 12800/14984] loss 0.8537 acc 0.7602 | 10631.2s elapsed\n",
      "  [batch 12900/14984] loss 0.8534 acc 0.7602 | 10715.5s elapsed\n",
      "  [batch 13000/14984] loss 0.8533 acc 0.7602 | 10799.4s elapsed\n",
      "  [batch 13100/14984] loss 0.8532 acc 0.7603 | 10883.3s elapsed\n",
      "  [batch 13200/14984] loss 0.8527 acc 0.7604 | 10967.9s elapsed\n",
      "  [batch 13300/14984] loss 0.8528 acc 0.7605 | 11051.9s elapsed\n",
      "  [batch 13400/14984] loss 0.8526 acc 0.7606 | 11135.7s elapsed\n",
      "  [batch 13500/14984] loss 0.8520 acc 0.7607 | 11220.1s elapsed\n",
      "  [batch 13600/14984] loss 0.8513 acc 0.7609 | 11305.3s elapsed\n",
      "  [batch 13700/14984] loss 0.8510 acc 0.7611 | 11389.8s elapsed\n",
      "  [batch 13800/14984] loss 0.8506 acc 0.7612 | 11474.7s elapsed\n",
      "  [batch 13900/14984] loss 0.8504 acc 0.7612 | 11558.8s elapsed\n",
      "  [batch 14000/14984] loss 0.8499 acc 0.7613 | 11643.3s elapsed\n",
      "  [batch 14100/14984] loss 0.8498 acc 0.7614 | 11727.9s elapsed\n",
      "  [batch 14200/14984] loss 0.8488 acc 0.7617 | 11812.6s elapsed\n",
      "  [batch 14300/14984] loss 0.8486 acc 0.7617 | 11897.1s elapsed\n",
      "  [batch 14400/14984] loss 0.8481 acc 0.7619 | 11981.7s elapsed\n",
      "  [batch 14500/14984] loss 0.8478 acc 0.7619 | 12066.9s elapsed\n",
      "  [batch 14600/14984] loss 0.8471 acc 0.7620 | 12151.1s elapsed\n",
      "  [batch 14700/14984] loss 0.8468 acc 0.7621 | 12235.5s elapsed\n",
      "  [batch 14800/14984] loss 0.8465 acc 0.7622 | 12319.3s elapsed\n",
      "  [batch 14900/14984] loss 0.8462 acc 0.7623 | 12404.3s elapsed\n",
      "Epoch 3/3 | train loss 0.8456 acc 0.7624 | val loss 0.7939 acc 0.7553\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7552870090634441"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca08b100",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from sklearn.metrics import classification_report, confusion_matrix\n",
    "    import seaborn as sns\n",
    "    SKLEARN_OK = True\n",
    "except Exception:\n",
    "    SKLEARN_OK = False\n",
    "\n",
    "val_loss, val_acc, val_preds, val_labels = evaluate(model, val_loader, criterion)\n",
    "test_loss, test_acc, test_preds, test_labels = evaluate(model, test_loader, criterion)\n",
    "print(f'Val acc: {val_acc:.4f} | Test acc: {test_acc:.4f}')\n",
    "\n",
    "if SKLEARN_OK:\n",
    "    report = classification_report(val_labels, val_preds, target_names=class_names, digits=4)\n",
    "    print(report)\n",
    "    cm = confusion_matrix(val_labels, val_preds)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, cmap='Blues', cbar=False)\n",
    "    plt.title('Validation Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.show()\n",
    "else:\n",
    "    print('sklearn not available; skipping classification report and confusion matrix.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad53a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def predict_one(path: Path):\n",
    "    img = datasets.folder.default_loader(path)\n",
    "    x = eval_tfms(img).unsqueeze(0).to(device)\n",
    "    logits = model(x)\n",
    "    probs = torch.softmax(logits, dim=1).squeeze(0)\n",
    "    topk = torch.topk(probs, k=5)\n",
    "    top_labels = [idx_to_class[i.item()] for i in topk.indices]\n",
    "    return list(zip(top_labels, topk.values.cpu().tolist()))\n",
    "\n",
    "# Example\n",
    "sample_path = next(iter(VAL_DIR.iterdir()))\n",
    "sample_path, predict_one(sample_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
